{"id": "0", "text": "This is a tiny demo document. It explains that LEANN is used for semantic search and retrieval for RAG.", "metadata": {"file_path": "data/test.txt", "file_name": "test.txt", "creation_date": "2026-01-11", "last_modified_date": "2026-01-11"}}
{"id": "1", "text": "Multiple Yield Curve Modeling and Forecasting using Deep\nLearning\nRonald Richman ∗ Salvatore Scognamiglio †\nVersion of January 31, 2024\nAbstract\nThis manuscript introduces deep learning models that simultaneously describe the dynamics\nof several yield curves. We aim to learn the dependence structure among the different\nyield curves induced by the globalization of financial markets and exploit it to produce\nmore accurate forecasts. By combining the self-attention mechanism and nonparametric\nquantile regression, our model generates both point and interval forecasts of future yields.\nThe architecture is designed to avoid quantile crossing issues affecting multiple quantile\nregression models. Numerical experiments conducted on two different datasets confirm the\neffectiveness of our approach. Finally, we explore potential extensions and enhancements by\nincorporating deep ensemble methods and transfer learning mechanisms.\nKeywords. Deep Learning, Multiple Yield Curve modeling, Nelson-Siegel model, Attention\nModels, Transfer Learning, Interest Rate Risk, Value-at-Risk, Asset-Liability Management,\nSolvency II, IFRS 17, Real-world modelling.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "2", "text": "The architecture is designed to avoid quantile crossing issues affecting multiple quantile\nregression models. Numerical experiments conducted on two different datasets confirm the\neffectiveness of our approach. Finally, we explore potential extensions and enhancements by\nincorporating deep ensemble methods and transfer learning mechanisms.\nKeywords. Deep Learning, Multiple Yield Curve modeling, Nelson-Siegel model, Attention\nModels, Transfer Learning, Interest Rate Risk, Value-at-Risk, Asset-Liability Management,\nSolvency II, IFRS 17, Real-world modelling.\n1 Introduction\nYield curves are used for a wide variety of tasks in actuarial science and finance for deriving\nthe present value of future cashflows within valuations that apply a market consistent approach.\nA market consistent approach is required by modern solvency regulations, such as Solvency II,\nwhile recently updated accounting standards, such as the recently introduced IFRS 17, require\nthe use of credit and liquidity adjusted yield curves for discounting liabilities, including both\nlife and non-life insurance liabilities.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "3", "text": "1 Introduction\nYield curves are used for a wide variety of tasks in actuarial science and finance for deriving\nthe present value of future cashflows within valuations that apply a market consistent approach.\nA market consistent approach is required by modern solvency regulations, such as Solvency II,\nwhile recently updated accounting standards, such as the recently introduced IFRS 17, require\nthe use of credit and liquidity adjusted yield curves for discounting liabilities, including both\nlife and non-life insurance liabilities. Insurers, and other entities, that report on their liabilities\non a discounted basis are exposed to the risk of changes in the interest rates in their markets,\nwhich translate directly into changes in the solvency of these entities. Therefore, managing\nthis risk of adverse changes in yield curves - which we refer to as interest rate risk in what\nfollows - is an important task within actuarial work, which is usually considered in the context\nof corresponding changes in the asset portfolio backing these liabilities, changes in the value\nof which may act as an offset.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "4", "text": "Insurers, and other entities, that report on their liabilities\non a discounted basis are exposed to the risk of changes in the interest rates in their markets,\nwhich translate directly into changes in the solvency of these entities. Therefore, managing\nthis risk of adverse changes in yield curves - which we refer to as interest rate risk in what\nfollows - is an important task within actuarial work, which is usually considered in the context\nof corresponding changes in the asset portfolio backing these liabilities, changes in the value\nof which may act as an offset. This process is, therefore, usually referred to as Asset-Liability\nManagement (ALM).", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "5", "text": "Therefore, managing\nthis risk of adverse changes in yield curves - which we refer to as interest rate risk in what\nfollows - is an important task within actuarial work, which is usually considered in the context\nof corresponding changes in the asset portfolio backing these liabilities, changes in the value\nof which may act as an offset. This process is, therefore, usually referred to as Asset-Liability\nManagement (ALM). Moreover, insurers are required to hold capital to ensure that their solvency\n∗Old Mutual Insure and University of the Witwatersrand, Johannesburg, South Africa; ronaldrich-\nman@gmail.com\n†Department of Management and Quantitative Studies, University of Naples “Parthenope”,\nsalvatore.scognamiglio@uniparthenope.it\n1\narXiv:2401.16985v1  [stat.ML]  30 Jan 2024", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "6", "text": "is adequately protected in most solvency regimes, such as Solvency II. To measure the extent\nof the interest rate risk, as well as the corresponding capital needed to be held, insurers and\nother financial institutions often rely on modelling the uncertain future evolution of the yield\ncurve using a variety of different models. Once the yield curves have been modelled, the models\nare used to derive scenarios for the future evolution of the yield curves, which are then applied\nto derive capital requirements. Here, we distinguish between unconditional and conditional\napproaches to yield curve modelling: the former approach calibrates models of the yield curve\nevolution using historical data at a point in time, derives stresses based on these, and then\napplies these stresses without recalibrating these based on current market conditions. This\napproach underlies, for example, the standard formula approach of the Solvency II regulation.\nOn the other hand, the conditional approach uses current market information to recalibrate yield\ncurve stresses; this approach is often taken in internal model approaches within the Solvency II\nregulation.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "7", "text": "Here, we distinguish between unconditional and conditional\napproaches to yield curve modelling: the former approach calibrates models of the yield curve\nevolution using historical data at a point in time, derives stresses based on these, and then\napplies these stresses without recalibrating these based on current market conditions. This\napproach underlies, for example, the standard formula approach of the Solvency II regulation.\nOn the other hand, the conditional approach uses current market information to recalibrate yield\ncurve stresses; this approach is often taken in internal model approaches within the Solvency II\nregulation.\nModelling interest rate risks is made more difficult due to the complexity of requirements of\nrecent accounting standards, as well as the interconnected nature of financial markets across\nasset classes and geographies. The recent IFRS 17 standard departs from a purely market\nconsistent valuation approach by requiring insurers to use yield curves that are modified to\ncorrespond to the financial characteristics of the liabilities being valued, as well as the asset\nportfolios backing these.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "8", "text": "Modelling interest rate risks is made more difficult due to the complexity of requirements of\nrecent accounting standards, as well as the interconnected nature of financial markets across\nasset classes and geographies. The recent IFRS 17 standard departs from a purely market\nconsistent valuation approach by requiring insurers to use yield curves that are modified to\ncorrespond to the financial characteristics of the liabilities being valued, as well as the asset\nportfolios backing these. While we do not explain this in detail, in short, insurers must derive\nyield curves consisting of the (credit) risk-free interest rate, as well as an allowance for an\nilliquidity premium. Thus, insurers reporting under IFRS 17 calibrate several different yield\ncurves for discounting liabilities, the evolution of which will differ depending on both how risk-\nfree rates and illiquidity premia change over time.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "9", "text": "While we do not explain this in detail, in short, insurers must derive\nyield curves consisting of the (credit) risk-free interest rate, as well as an allowance for an\nilliquidity premium. Thus, insurers reporting under IFRS 17 calibrate several different yield\ncurves for discounting liabilities, the evolution of which will differ depending on both how risk-\nfree rates and illiquidity premia change over time. Another reason that insurers may need to\nmodel the evolution of multiple yield curves is due to their investing in assets with different\nlevels of credit quality; to manage the risk of the asset portfolio, it is often necessary to calibrate\nmultiple yield curves which take account of credit-risk premia and model the (joint) evolution\nof these. Finally, insurers operating in multiple geographic jurisdictions need to manage the\ninterest rate risk arising from changes in the different yield curves used in these markets.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "10", "text": "Another reason that insurers may need to\nmodel the evolution of multiple yield curves is due to their investing in assets with different\nlevels of credit quality; to manage the risk of the asset portfolio, it is often necessary to calibrate\nmultiple yield curves which take account of credit-risk premia and model the (joint) evolution\nof these. Finally, insurers operating in multiple geographic jurisdictions need to manage the\ninterest rate risk arising from changes in the different yield curves used in these markets. In\nall of these scenarios, it is not sufficient merely to model the dynamics of each yield curve on\nan independent basis, since this approach will not capture the correlation between asset classes\nand geographies and may lead to misstated estimates of risk and capital; rather the joint future\nevolution of the complete set of yield curves used by the insurer must be modelled.\nIn this work, we focus on exactly this problem of jointly modelling and forecasting multiple\nyield curves for interest rate risk management, ALM and derivation of capital requirements; we\nnote that this is done on the real-world basis and not the risk-neutral basis which is useful for\noption valuation.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "11", "text": "In this work, we focus on exactly this problem of jointly modelling and forecasting multiple\nyield curves for interest rate risk management, ALM and derivation of capital requirements; we\nnote that this is done on the real-world basis and not the risk-neutral basis which is useful for\noption valuation. For this task, we use neural network models trained jointly on a significant\namount of historical yield curve data across geographies to forecast yield curves on an expected\n(best-estimate) basis, as well as to forecast the quantiles of the yield curves; the latter can be\nused directly for risk management purposes, for example, calculating the Value-at-Risk of the\ninsurer.\nLiterature review\nSeveral different approaches have appeared in the literature to model the uncertain future evo-\nlution of the yield curve [39, 41].\n2", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "12", "text": "One class of models - focused on the risk-neutral evolution of the yield curve - consists of\narbitrage-free models, which impose restrictions on the evolution of the yield curve to avoid\nrisk-free profit opportunities. Prominent examples are the models developed in [22] and [20].\nAlthough these models are widely used in option pricing, without further adaptation, they are\noften found to forecast poorly compared with a simple random walk model (see [12]). We refer\nto [43], and the citations therein, for interesting discussions of adapting risk-neutral interest rate\nmodels for real-world purposes by estimating the market-price of interest rate risk. A popular\ncommercial approach used by some insurers for internal modelling of interest rate (and other\nmarket) risks in the Solvency II capital regime consists of modifying arbitrage-free models to\nensure that the implied future evolution of yield curves is constrained to meet certain real-world\neconomic and market-variable targets, however, there is relatively little discussion of this in the\nacademic literature.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "13", "text": "We refer\nto [43], and the citations therein, for interesting discussions of adapting risk-neutral interest rate\nmodels for real-world purposes by estimating the market-price of interest rate risk. A popular\ncommercial approach used by some insurers for internal modelling of interest rate (and other\nmarket) risks in the Solvency II capital regime consists of modifying arbitrage-free models to\nensure that the implied future evolution of yield curves is constrained to meet certain real-world\neconomic and market-variable targets, however, there is relatively little discussion of this in the\nacademic literature.\nA well-known technique is to apply principal components analysis (PCA) to vectors of the\nchanges in the yield curve at each term and then to use the simulated changes in the yield curve\nto derive a distribution of yield curves (for an overview, see [32]). This approach was used, for\nexample, to calibrate the yield curve stresses in the interest-rate risk module of Solvency II.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "14", "text": "A well-known technique is to apply principal components analysis (PCA) to vectors of the\nchanges in the yield curve at each term and then to use the simulated changes in the yield curve\nto derive a distribution of yield curves (for an overview, see [32]). This approach was used, for\nexample, to calibrate the yield curve stresses in the interest-rate risk module of Solvency II. This\nis an example of the unconditional approach to yield curve modelling, since the PCA analysis\nwas performed at a point in time in the past, and is assumed to still be relevant in current\nmarket conditions.\nOther authors follow a purely statistical approach. This class of models has evolved from\nunivariate [13] to multivariate time series models and recent advances in dynamic factor models.\nAmong them, the dynamic extension of the well-known Nelson–Siegel (NS) model [26] proposed\nin [9] (from now on referred to as DNS) has become very popular among practitioners thanks\nto its simplicity and discrete forecasting ability.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "15", "text": "Other authors follow a purely statistical approach. This class of models has evolved from\nunivariate [13] to multivariate time series models and recent advances in dynamic factor models.\nAmong them, the dynamic extension of the well-known Nelson–Siegel (NS) model [26] proposed\nin [9] (from now on referred to as DNS) has become very popular among practitioners thanks\nto its simplicity and discrete forecasting ability. In addition to being a relatively simple and\nparsimonious model, the DNS approach is also appealing since the NS model (and its extensions)\nunderlying the DNS approach is often used by central banks and other institutions for calibrating\nthe yield curve. Other notable examples of factor models can be found in [6, 19].\nNumerous extensions of the DNS approach have been developed in the literature. Some resarch\ninvestigated using more flexible versions of the Nelson-Siegel model, for example, the model\nproposed in [5], the four-factor extension suggested in [38] and the five-factor model investigated\nin [8].", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "16", "text": "Other notable examples of factor models can be found in [6, 19].\nNumerous extensions of the DNS approach have been developed in the literature. Some resarch\ninvestigated using more flexible versions of the Nelson-Siegel model, for example, the model\nproposed in [5], the four-factor extension suggested in [38] and the five-factor model investigated\nin [8]. Other authors try to improve the forecasting performance of the DNS model, including\nsome macroeconomics variables in the models (see [11]). A nice overview of the NS and DNS\nmodels is in [10], who also provide economic intuitions for the factors used in the NS model.\nMultiple yield curve modeling\nGlobalization has intensified the connection among the financial markets, inducing a dependence\nstructure among different yield curves, which renders the process of modelling these jointly\ncomplex; moreover, above we have discussed other reasons for the need for joint modelling\nof multiple yield curves, which is a challenging task.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "17", "text": "Other authors try to improve the forecasting performance of the DNS model, including\nsome macroeconomics variables in the models (see [11]). A nice overview of the NS and DNS\nmodels is in [10], who also provide economic intuitions for the factors used in the NS model.\nMultiple yield curve modeling\nGlobalization has intensified the connection among the financial markets, inducing a dependence\nstructure among different yield curves, which renders the process of modelling these jointly\ncomplex; moreover, above we have discussed other reasons for the need for joint modelling\nof multiple yield curves, which is a challenging task. Despite the relevance of the topic for\nfinancial markets, there is relatively little discussion of this topic in the literature on a real-\nworld basis; on the other hand, more literature is available in the risk-neutral setting, see [7] for\nan overview. Within the real-work setting, here we mention [15], who introduced a multiple-\ncurves PCA method where the dynamics of multiple yield curves captured through Principal\nComponent Analysis (PCA) are modelled as autoregressive processes and [2], who proposed a\n3", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "18", "text": "two-step method to jointly capture the risk-factor relationships within each curve and the risk-\nfactor relationships between the curves. In a first stage, the authors use the PCA to derives\ncomponents describing the dynamics of each curve, and then, secondly, combine these to describe\nthe dynamics across all the curves. Notably, the joint forecasting of best-estimates and quantiles\nis not done in these works, and this is a novel aspect of the model presented here.\nNeural networks for yield curve modeling\nRecently, deep learning models have become popular for general machine learning tasks, due\nto their ability to model massive volumes of data in a flexible manner, and within finance and\nactuarial science. Deep neural networks have been successfully applied to several tasks such\nas pricing [3, 28], reserving [14] and mortality forecasting [30, 36]. A detailed overview of the\napplication of Artificial Intelligence (AI) and Machine Learning (ML) techniques in actuarial\nscience can be found in [33, 34].", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "19", "text": "Neural networks for yield curve modeling\nRecently, deep learning models have become popular for general machine learning tasks, due\nto their ability to model massive volumes of data in a flexible manner, and within finance and\nactuarial science. Deep neural networks have been successfully applied to several tasks such\nas pricing [3, 28], reserving [14] and mortality forecasting [30, 36]. A detailed overview of the\napplication of Artificial Intelligence (AI) and Machine Learning (ML) techniques in actuarial\nscience can be found in [33, 34]. Focusing on yield curves modeling and interest rate risk\nmanagement, the literature is relatively sparse. [1] shows that feed-forward NN can be used to\nreplace time series models to extrapolate the future values of the NS parameters. [23] proposes to\nimprove the flexibility of the DNS model using NN for deriving the factor loadings.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "20", "text": "A detailed overview of the\napplication of Artificial Intelligence (AI) and Machine Learning (ML) techniques in actuarial\nscience can be found in [33, 34]. Focusing on yield curves modeling and interest rate risk\nmanagement, the literature is relatively sparse. [1] shows that feed-forward NN can be used to\nreplace time series models to extrapolate the future values of the NS parameters. [23] proposes to\nimprove the flexibility of the DNS model using NN for deriving the factor loadings. [29] directly\nemploy feed-forward NN to forecast future yields, and [16] use modern recurrent neural networks\nsuch as the Long Short-Term Memory which are specifically designed to analyse sequential data,\nsuch as the time-series of the yields.\nContributions\nIn this work, we develop deep learning models that simultaneously model and forecast the dy-\nnamics of the multiple yield curves, which could be related to different countries, credit qualities\nor liquidity characteristics; here, we focus on the first two of these.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "21", "text": "[29] directly\nemploy feed-forward NN to forecast future yields, and [16] use modern recurrent neural networks\nsuch as the Long Short-Term Memory which are specifically designed to analyse sequential data,\nsuch as the time-series of the yields.\nContributions\nIn this work, we develop deep learning models that simultaneously model and forecast the dy-\nnamics of the multiple yield curves, which could be related to different countries, credit qualities\nor liquidity characteristics; here, we focus on the first two of these. With their ability to de-\nscribe high-dimensional time-series data and model the non-linearity often present in the data,\ndeep learning techniques are promising tools for multiple yield curve modeling and forecasting.\nThe idea is to exploit the dependence structure among the different yield curves induced by the\nglobalization of financial markets or relationships between asset classes to improve the forecast-\ning performance of our models by jointly training these on historical datasets of yield curves.\nImportantly, we focus both on producing best-estimate forecasts of the yield curve, as we as em-\nploy deep learning techniques to quantify the uncertainty around the predictions by forecasting\nquantiles.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "22", "text": "With their ability to de-\nscribe high-dimensional time-series data and model the non-linearity often present in the data,\ndeep learning techniques are promising tools for multiple yield curve modeling and forecasting.\nThe idea is to exploit the dependence structure among the different yield curves induced by the\nglobalization of financial markets or relationships between asset classes to improve the forecast-\ning performance of our models by jointly training these on historical datasets of yield curves.\nImportantly, we focus both on producing best-estimate forecasts of the yield curve, as we as em-\nploy deep learning techniques to quantify the uncertainty around the predictions by forecasting\nquantiles. Although this latter aspect is relevant both from a practical risk management and a\ntheoretical point of view, it has not been deeply investigated in the literature, and creating joint\nforecasts of these is, to our knowledge, novel. For uncertainty quantification we use modern deep\nlearning approaches, such as non-parametric quantile regression and deep ensembles model [25].", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "23", "text": "Importantly, we focus both on producing best-estimate forecasts of the yield curve, as we as em-\nploy deep learning techniques to quantify the uncertainty around the predictions by forecasting\nquantiles. Although this latter aspect is relevant both from a practical risk management and a\ntheoretical point of view, it has not been deeply investigated in the literature, and creating joint\nforecasts of these is, to our knowledge, novel. For uncertainty quantification we use modern deep\nlearning approaches, such as non-parametric quantile regression and deep ensembles model [25].\nThis augmentation of best-estimate forecast models with forecast quantiles is particularly useful\nfor risk management purposes since the quantiles correspond to the VaR, which, in practice,\nunderlies many quantitative risk management systems in practice. A comparison among these\nmethods for measuring the uncertainty in the forecasts is also of interest, since it could provide\nadditional insights into how well these methods describe the evolution of the yield curves.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "24", "text": "For uncertainty quantification we use modern deep\nlearning approaches, such as non-parametric quantile regression and deep ensembles model [25].\nThis augmentation of best-estimate forecast models with forecast quantiles is particularly useful\nfor risk management purposes since the quantiles correspond to the VaR, which, in practice,\nunderlies many quantitative risk management systems in practice. A comparison among these\nmethods for measuring the uncertainty in the forecasts is also of interest, since it could provide\nadditional insights into how well these methods describe the evolution of the yield curves.\nWe utilize recent advances in deep learning methodology into our selected yield curve model,\nspecifically, the self-attention mechanism, which has been used to great success in natural lan-\nguage processing [40] and has recently been applied for severity modeling of flood insurance\n4", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "25", "text": "claims [24]. Here, we show how the features derived using a convolutional neural network can\nbe enhanced using the self-attention mechanism for greater forecasting accuracy.\nFinally, we investigate the use of transfer learning methods, which aim to transfer as much\nknowledge as possible from an existing model to a new model designed for a similar task. These\nmethods are already intensively used in computer vision and natural language processing tasks,\nwhere models are trained on large general datasets, then fine-tuned on more specific tasks. In\nour context, the transfer learning mechanism is applied to exploit knowledge learned by param-\neterizing models on a database of yield curves from multiple jurisdictions, then transferring the\nlearned model to a smaller dataset of yield curves derived for assets of various credit quality.\nOrganization of the manuscript: The rest of the manuscript is structured as follows. Section\n2 introduces two of the most popular factor models for yield curve modeling and forecasting,\nSection 3 describes neural network building blocks used in Section 4, where we present the\nproposed yield curve model.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "26", "text": "In\nour context, the transfer learning mechanism is applied to exploit knowledge learned by param-\neterizing models on a database of yield curves from multiple jurisdictions, then transferring the\nlearned model to a smaller dataset of yield curves derived for assets of various credit quality.\nOrganization of the manuscript: The rest of the manuscript is structured as follows. Section\n2 introduces two of the most popular factor models for yield curve modeling and forecasting,\nSection 3 describes neural network building blocks used in Section 4, where we present the\nproposed yield curve model. Section 5 illustrates some numerical experiments on a large cross-\ngeography dataset of yield curves, Section 6 discusses some possible ways to extend and enhance\nthe proposed model, and Section 7 concludes.\n2 Dynamic Nelson-Siegel and Nelson-Siegel-Svenson Models\nWe consider a scenario where the objective is to model the dynamics of yield curves belong-\ning to different families. These curve families might pertain to yield curves that vary in credit\nrating quality; for example, families could be labeled as ‘A’, ‘AA’, ‘AAA’ and so forth.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "27", "text": "Section 5 illustrates some numerical experiments on a large cross-\ngeography dataset of yield curves, Section 6 discusses some possible ways to extend and enhance\nthe proposed model, and Section 7 concludes.\n2 Dynamic Nelson-Siegel and Nelson-Siegel-Svenson Models\nWe consider a scenario where the objective is to model the dynamics of yield curves belong-\ning to different families. These curve families might pertain to yield curves that vary in credit\nrating quality; for example, families could be labeled as ‘A’, ‘AA’, ‘AAA’ and so forth. Alter-\nnatively, the yield curves could be associated with government bonds from different countries,\nwith families labeled as ‘Euro’, ‘United Kingdom’ and so on.\nLet I represent the set of considered curve families, M denote the set of time-to-maturities\nfor which the curve is defined, and y(i)\nt (τ) denote the continuously compounded zero-coupon\nnominal yield at time t ∈ T on a τ-month bond (i.e.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "28", "text": "Alter-\nnatively, the yield curves could be associated with government bonds from different countries,\nwith families labeled as ‘Euro’, ‘United Kingdom’ and so on.\nLet I represent the set of considered curve families, M denote the set of time-to-maturities\nfor which the curve is defined, and y(i)\nt (τ) denote the continuously compounded zero-coupon\nnominal yield at time t ∈ T on a τ-month bond (i.e. at tenor τ ∈ M ) for the i-th bond in\nthe set I. Importantly, we note that here we work with spot rates, whereas, for example, PCA\nanalysis of yield curves is often performed on forward rates. Here we focus on describing more\ntraditional models which will be used as a benchmark for the neural network models introduced\nlater.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "29", "text": "at tenor τ ∈ M ) for the i-th bond in\nthe set I. Importantly, we note that here we work with spot rates, whereas, for example, PCA\nanalysis of yield curves is often performed on forward rates. Here we focus on describing more\ntraditional models which will be used as a benchmark for the neural network models introduced\nlater.\nIn their influential work, Nelson and Siegel (NS) [26] introduce a three-factor model that, at\na given date t, describes the relationship between the yield and maturity τ. Given that the\nclassical NS model is static, [9] introduces a dynamic version where the model’s parameters can\nvary over time.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "30", "text": "In this case, y(i)\nt (τ) can be expressed as follows:\ny(i)\nt (τ) = β(i)\n0,t + β(i)\n1,t\n\u00101 − e−λ(i)\nt τ\nλ(i)\nt τ\n\u0011\n+ β(i)\n2,t\n\u00101 − e−λ(i)\nt τ\nλ(i)\nt τ\n− e−λ(i)\nt τ\n\u0011\n+ ϵ(i)\nt (τ), ϵ (i)\nt (τ) ∼ N (0, σ2\nϵ(i)),\nwhere β(i)\n0,t, β(i)\n1,t, β(i)\n2,t, λ(i)\nt ∈ R are model parameters governing the shape of the curve that are\nestimated for each date t and each family curve i by using market data.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "31", "text": "More specifically, the\nparameters β(i)\nj,t with j ∈ {0, 1, 2} can be interpreted as three latent factor factors defining the\nlevel, slope and curvature of the yield curve, respectively, while λ(i)\nt indicates where the loading\nachieves its maximum; here, we follow the interpretation of these factors given in [9]. The\ncalibration of the NS model with respect to all the parameters raises an optimization problem\n5", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "32", "text": "that is intrinsically nonlinear due to the λ(i)\nt parameter. However, since it doesn’t impact the\nresults, many authors, including [9], suggest keeping λ(i)\nt = λ(i), t ∈ T fixed, and estimating the\nremaining parameters by solving, for each family i, the sequence of linear optimization problems:\narg min\nβ(i)\n0,t,β(i)\n1,t,β(i)\n2,t\nX\nτ ∈M\n\u0012\ny(i)\nt (τ) − β(i)\n0,t − β(i)\n1,t\n\u00101 − e−λiτ\nλ(i)\nt τ\n\u0011\n− β(i)\n2,t\n\u00101 − e−λ(i)\nt τ\nλ(i)\nt τ\n− e−λ(i)\nt τ\n\u0011\u00132\n, ∀t ∈ T .\nTo make forecasts, a dynamic model for the latent factors β(i)\nj,t with j ∈ { 0, 1, 2} has to be\nspecified.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "33", "text": "To make forecasts, a dynamic model for the latent factors β(i)\nj,t with j ∈ { 0, 1, 2} has to be\nspecified. The simplest choice consists of using a set of individual first-order Autoregressive\n(AR) models:\nβ(i)\nj,t = ψ0, j(i) + ψ(i)\n1,jβ(i)\nj,t−1 + ζ (i)\nt , ζ (i)\nt ∼ N (0, σ2\nζ(i))\nwhere ψ(i)\n0,j, ψ(i)\n1,j ∈ R, i ∈ I , j ∈ { 0, 1, 2} are the time-series model parameters and ζ (i)\nt are nor-\nmally distributed error terms.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "34", "text": "Alternatively, one could also model the vectorβ(i)\nt =\n\u0000\nβ(i)\n0,t, β(i)\n1,t, β(i)\n2,t\n\u0001\n∈\nR3 using single first-order multivariate Vector Autoregressive (VAR) model:\nβ(i)\nt = a(i)\n0 + A(i)β(i)\nt−1 + η(i)\nt , η(i)\nt ∼ N (0, E(i))\nwith a(i)\n0 ∈ R3, A(i) ∈ R3×3, and η(i)\nt ∼ N(0, E(i)) is the normally distributed error term with\nmatrix E(i) ∈ R3×3.\nNumerous extensions of the NS model and its dynamic version have been proposed in the liter-\nature. One of the most popular enhancements, due to Svensson [38], introduces an additional\nterm to augment flexibility. This extension enhances the model’s capacity to capture various\nshapes of yield curves by incorporating additional curvature components.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "35", "text": "Numerous extensions of the NS model and its dynamic version have been proposed in the liter-\nature. One of the most popular enhancements, due to Svensson [38], introduces an additional\nterm to augment flexibility. This extension enhances the model’s capacity to capture various\nshapes of yield curves by incorporating additional curvature components. The Svensson exten-\nsion allows for a more general representation of the term structure of interest rates, establishing\nit as a valuable and often used tool in fixed-income and financial modeling.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "36", "text": "The Svensson exten-\nsion allows for a more general representation of the term structure of interest rates, establishing\nit as a valuable and often used tool in fixed-income and financial modeling. The Nelson-Siegel-\nSvensson (NSS) model is defined as follows:\ny(i)\nt (τ) = β(i)\n0,t+β(i)\n1,t\n\u00101 − e−λ(i)\n1,tτ\nλ(i)\n1,tτ\n\u0011\n+β(i)\n2,t\n\u00101 − e−λ(i)\n1,tτ\nλ(i)\n1,tτ\n−e−λ(i)\n1,tτ\n\u0011\n+β(i)\n3,t\n\u00101 − e−λ(i)\n2,tτ\nλ(i)\n2,tτ\n−e−λ(i)\n2,tτ\n\u0011\n+ϵ(i)\nt (τ),\nwhere β(i)\n3,t ∈ R is a second curvature parameter, and λ(i)\n1,t, λ(i)\n2,t ∈ R are two decay factors.\nForecasts are obtained by applying the procedure adopted by [9].", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "37", "text": "Forecasts are obtained by applying the procedure adopted by [9].\n3 Neural Networks\nNeural networks (NN) represent nonlinear statistical models originally inspired by the function-\ning of the human brain, as implied by their name, and subsequently extensively developed for\nmultiple applications in machine learning, see [17] for a review. A feed-forward neural network\ncomprises interconnected computational units, or neurons, organized in multiple layers. These\nneurons “learn” from data through training algorithms. The fundamental concept involves\nmapping input data to a new multi-dimensional space, extracting derived features. The output\n(target) is then modeled as a nonlinear function of these derived features; this process is called\n6", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "38", "text": "representation learning [4]. Implementing multiple feed-forward network layers is called deep\nlearning in the literature, and has proved to be particularly promising when dealing with high-\ndimensional problems requiring the identification of nonlinear dependencies. The arrangement\nof connections among the units delineates various types of neural networks. Our neural network\nmodel is constructed on the principles of both feed-forward and recurrent neural networks. An\noverview of the neural network blocks employed in the best performing model presented in this\npaper is provided below, whereas we summarize briefly the network blocks that are used in\nmodels that perform less well. For more detail on neural networks in an actuarial context, we\nrefer to [42], whose notation we follow.\n3.1 Fully-Connected Layer\nA fully connected network (FCN) layer, also commonly known as a dense layer due to the dense\nconnections between units, is a type of layer in a neural network where each neuron or unit\nis connected to every neuron in the previous layer.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "39", "text": "An\noverview of the neural network blocks employed in the best performing model presented in this\npaper is provided below, whereas we summarize briefly the network blocks that are used in\nmodels that perform less well. For more detail on neural networks in an actuarial context, we\nrefer to [42], whose notation we follow.\n3.1 Fully-Connected Layer\nA fully connected network (FCN) layer, also commonly known as a dense layer due to the dense\nconnections between units, is a type of layer in a neural network where each neuron or unit\nis connected to every neuron in the previous layer. In other words, each neuron in a fully\nconnected layer receives input from all the neurons in the preceding layer; if the layer is the\nfirst in a network, then each neuron is receives inputs from all of the covariates input into the\nnetwork.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "40", "text": "3.1 Fully-Connected Layer\nA fully connected network (FCN) layer, also commonly known as a dense layer due to the dense\nconnections between units, is a type of layer in a neural network where each neuron or unit\nis connected to every neuron in the previous layer. In other words, each neuron in a fully\nconnected layer receives input from all the neurons in the preceding layer; if the layer is the\nfirst in a network, then each neuron is receives inputs from all of the covariates input into the\nnetwork.\nLet x ∈ Rq0 be the input vector; a FCN layer with q1 ∈ N units is a vector function that maps\nx into a q1-dimensional real-valued space:\nz(1) : Rq0 → Rq1, x 7→ z(1)(x) =\n\u0010\nz(1)\n1 (x), z(1)\n2 (x), . . . , z(1)\nq1 (x)\n\u0011′\n.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "41", "text": "Let x ∈ Rq0 be the input vector; a FCN layer with q1 ∈ N units is a vector function that maps\nx into a q1-dimensional real-valued space:\nz(1) : Rq0 → Rq1, x 7→ z(1)(x) =\n\u0010\nz(1)\n1 (x), z(1)\n2 (x), . . . , z(1)\nq1 (x)\n\u0011′\n.\nThe output of each unit is a new feature z(1)\nj (x), which is a non-linear function of x:\nz(1)\nj (x) = ϕ\n\u0012\nw(1)\nj,0 +\nq1X\nl=1\nw(1)\nj,l xl\n\u0013\nj = 1, 2, ..., q1,\nwhere ϕ : R 7→ R is the activation function and w(1)\nj,l ∈ R represent the weights.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "42", "text": ". . , z(1)\nq1 (x)\n\u0011′\n.\nThe output of each unit is a new feature z(1)\nj (x), which is a non-linear function of x:\nz(1)\nj (x) = ϕ\n\u0012\nw(1)\nj,0 +\nq1X\nl=1\nw(1)\nj,l xl\n\u0013\nj = 1, 2, ..., q1,\nwhere ϕ : R 7→ R is the activation function and w(1)\nj,l ∈ R represent the weights. In matrix form,\nthe output z(1)(x) of the FCN layer can be written as:\nz(1)(x) = ϕ\n\u0000\nw(1)\n0 + W (1)x\n\u0001\n. (3.1)\nShallow neural networks are those networks with a single dense layer and directly use the features\nderived in the layer for computing the (output) quantity of interest y ∈ Y .", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "43", "text": "In matrix form,\nthe output z(1)(x) of the FCN layer can be written as:\nz(1)(x) = ϕ\n\u0000\nw(1)\n0 + W (1)x\n\u0001\n. (3.1)\nShallow neural networks are those networks with a single dense layer and directly use the features\nderived in the layer for computing the (output) quantity of interest y ∈ Y . In the case of Y ⊆ R,\nthe output of shallow NN reads:\ny = ϕ\n\u0010\nw(o)\n0 + ⟨w(o), z(1)(x)⟩\n\u0011\n,\nwhere w(o)\n0 ∈ R, w(o) ∈ Rq1, ⟨·, ·⟩ denotes the scalar product in Rq1.\nIf, on the other hand, the network is deep, the vector z(1)(x) is used as input in the next layer\nfor computing new features and so on for the following layers.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "44", "text": "In the case of Y ⊆ R,\nthe output of shallow NN reads:\ny = ϕ\n\u0010\nw(o)\n0 + ⟨w(o), z(1)(x)⟩\n\u0011\n,\nwhere w(o)\n0 ∈ R, w(o) ∈ Rq1, ⟨·, ·⟩ denotes the scalar product in Rq1.\nIf, on the other hand, the network is deep, the vector z(1)(x) is used as input in the next layer\nfor computing new features and so on for the following layers. Let h ∈ N be the number of\nhidden layers (depth of network), and qk ∈ N, for 1 ≤ k ≤ h, be a sequence of integers that\nindicates the dimension of each FCN layer (widths of layers). A deep FCN can be described as\nfollows:\nx 7→ z(h:1)(x) =\n\u0010\nz(h) ◦ · · · ◦ z(1)\n\u0011\n(x) ∈ Rqh,\n7", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "45", "text": "where the vector functionsz(k) : Rqk−1 → Rqk have the same structure, andW (k) = (w(k)\nj )1≤j≤qk ∈\nRqk×qk−1, w(k)\n0 ∈ Rqk, for 1 ≤ k ≤ h are the network weights. In the case of deep NN, the output\nlayer uses the features extracted by the last hidden layer z(h:1)(x) instead of those z(1)(x).\nFinally, we mention that if the inputs to the FCN have a sequential structure, one way of\nprocessing these is to apply the same FCN to each input in turn, producing learned features\nfor each entry in the sequence. This is called a point-wise neural network in [40] and a time-\ndistributed network in the Keras library.\n3.2 Embedding Layer\nAn embedding layer is designed to acquire a low-dimensional representation of categorical vari-\nable levels. Let qL ∈ N denote the hyperparameter determining the size of the embedding.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "46", "text": "Finally, we mention that if the inputs to the FCN have a sequential structure, one way of\nprocessing these is to apply the same FCN to each input in turn, producing learned features\nfor each entry in the sequence. This is called a point-wise neural network in [40] and a time-\ndistributed network in the Keras library.\n3.2 Embedding Layer\nAn embedding layer is designed to acquire a low-dimensional representation of categorical vari-\nable levels. Let qL ∈ N denote the hyperparameter determining the size of the embedding.\nThe categorical variable levels are transformed into a real-valued qL-dimensional space, and the\ncoordinates of each level in this new space serve as learned parameters of the neural network,\nrequiring training; see [18] who introduced this technique for deep learning models.\nThe distances between levels in this learned space reflect the similarity of levels concerning\nthe target variable: closely related levels exhibit small Euclidean distances, while significantly\ndifferent categories display larger distances.\nFormally, let L = l1, l2, . . .", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "47", "text": "Let qL ∈ N denote the hyperparameter determining the size of the embedding.\nThe categorical variable levels are transformed into a real-valued qL-dimensional space, and the\ncoordinates of each level in this new space serve as learned parameters of the neural network,\nrequiring training; see [18] who introduced this technique for deep learning models.\nThe distances between levels in this learned space reflect the similarity of levels concerning\nthe target variable: closely related levels exhibit small Euclidean distances, while significantly\ndifferent categories display larger distances.\nFormally, let L = l1, l2, . . . , lnL represent the set of categories for the qualitative variable, and\nnL denote its cardinality. The embedding layer functions as a mapping\nzL : L → RqL.\nThe total number of embedding weights to be learned during training is nLqL.\n3.3 Attention Layer\nAn attention layer is a component in neural network architectures that implements an attention\nmechanism, which, in some sense “focuses” on certain aspects of the input data that are deemed\nto be relevant for the problem at hand.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "48", "text": ". . , lnL represent the set of categories for the qualitative variable, and\nnL denote its cardinality. The embedding layer functions as a mapping\nzL : L → RqL.\nThe total number of embedding weights to be learned during training is nLqL.\n3.3 Attention Layer\nAn attention layer is a component in neural network architectures that implements an attention\nmechanism, which, in some sense “focuses” on certain aspects of the input data that are deemed\nto be relevant for the problem at hand. These mechanisms allow the flexibility and performance\nof models to be enhanced and have produced excellent results, especially in tasks involving\nsequences like natural language processing, as well as within actuarial tasks, see, for example,\n[24]. The main idea of attention mechanisms is their ability to enable deep neural networks to\nreweight the significance of input data entering the model dynamically.\nSeveral attention mechanisms have been proposed in the literature.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "49", "text": "These mechanisms allow the flexibility and performance\nof models to be enhanced and have produced excellent results, especially in tasks involving\nsequences like natural language processing, as well as within actuarial tasks, see, for example,\n[24]. The main idea of attention mechanisms is their ability to enable deep neural networks to\nreweight the significance of input data entering the model dynamically.\nSeveral attention mechanisms have been proposed in the literature. We focus on the most\npopular form of attention, which is the scaled dot-product attention proposed in [40] for as a\ncomponent of the Transformer model proposed there.\nLet Q ∈ Rq×d be a matrix of query vectors, K ∈ Rq×d be a matrix of key vectors, V ∈ Rq×d is\na matrix of value vectors. The scaled dot-product attention mechanism is a mapping:\nA : R(q×d)×(q×d)×(q×d) → R(q×d), (Q, K, V ) 7→ A = attn(Q, K, V).", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "50", "text": "Let Q ∈ Rq×d be a matrix of query vectors, K ∈ Rq×d be a matrix of key vectors, V ∈ Rq×d is\na matrix of value vectors. The scaled dot-product attention mechanism is a mapping:\nA : R(q×d)×(q×d)×(q×d) → R(q×d), (Q, K, V ) 7→ A = attn(Q, K, V).\nThe attention mechanism is applied to the matrix V , and the resulting output is calculated as\na weighted sum of its elements. These attention coefficients depend on the matrices Q and K.\nThey undergo scalar-dot multiplication first, followed by the application of the softmax function\nto normalize the scores. Formally, the attention mapping has the following structure:\n8", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "51", "text": "A = softmax(B)V = softmax\n\u0012 QK⊤\n√\nd\n\u0013\nV\nwhere d ∈ [0, +∞) is a scalar coefficient, and the matrix of the scores B∗ is derived from the\nmatrix B:\nB∗ = softmax(B) where b ∗\ni,j = exp(bi,j)Pq\nk=1 exp(bi,k) ∈ (0, 1).\nd can be set equal to the dimension of the value vectors in the attention calculation, or can\nalternatively be a learned parameter. The scalar dot attention mechanism is computationally\nefficient, mainly because it does not require recursive computation and is thus easily implemented\non Graphics Processing Units (GPU), and has been widely adopted in Transformer-based models\ndue to its simplicity and effectiveness in capturing relationships between elements in a sequence.\nWe provide a simple intuition for the learned attention scores in B∗, which are multiplied by\nthe value vectors in V .", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "52", "text": "d can be set equal to the dimension of the value vectors in the attention calculation, or can\nalternatively be a learned parameter. The scalar dot attention mechanism is computationally\nefficient, mainly because it does not require recursive computation and is thus easily implemented\non Graphics Processing Units (GPU), and has been widely adopted in Transformer-based models\ndue to its simplicity and effectiveness in capturing relationships between elements in a sequence.\nWe provide a simple intuition for the learned attention scores in B∗, which are multiplied by\nthe value vectors in V . Each row of the new matrix B∗V is comprised of a weighted average of\nthe vectors in V , where the weights - adding to unity - determine the “importance” of each row\nvector i of V .\n3.4 Other network layers\nHere we briefly other neural network layers that were tested when designing the attention net-\nwork that is the main result presented in Section 4.\nThe network components presented to this point process inputs to the network without any\nreference to previous inputs to the network.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "53", "text": "Each row of the new matrix B∗V is comprised of a weighted average of\nthe vectors in V , where the weights - adding to unity - determine the “importance” of each row\nvector i of V .\n3.4 Other network layers\nHere we briefly other neural network layers that were tested when designing the attention net-\nwork that is the main result presented in Section 4.\nThe network components presented to this point process inputs to the network without any\nreference to previous inputs to the network. Recurrent Neural Networks (RNNs) modify neural\nnetworks to maintain an internal state that is calibrated from previous inputs of the network. An\nexample of an RNN is the Long Short Term Memory (LSTM) network of [21], which updates its\ninternal state using sub-networks that either “update” the internal state or “forget” information\nthat has already been learned.\nConvolutional Neural Networks (CNNs) differ from FCNs by connecting the units of this network\nonly to a small patch of the inputs, whereas the units in FCNs are connected to all of the inputs.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "54", "text": "Recurrent Neural Networks (RNNs) modify neural\nnetworks to maintain an internal state that is calibrated from previous inputs of the network. An\nexample of an RNN is the Long Short Term Memory (LSTM) network of [21], which updates its\ninternal state using sub-networks that either “update” the internal state or “forget” information\nthat has already been learned.\nConvolutional Neural Networks (CNNs) differ from FCNs by connecting the units of this network\nonly to a small patch of the inputs, whereas the units in FCNs are connected to all of the inputs.\nIn a CNN, the same network weights are applied to each patch.\nFinally, a Transformer model [40] builds on the self-attention layer in two main ways: first,\ninstead of applying self-attention once, multi-head attention is used to derive several versions of\nthe attention matrix A which are then compressed into a single matrix, and second, a point-wise\nneural network is used to process the compressed outputs from the multi-head attention.\nWe refer to [17] and [42] for more detail on these network layers.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "55", "text": "In a CNN, the same network weights are applied to each patch.\nFinally, a Transformer model [40] builds on the self-attention layer in two main ways: first,\ninstead of applying self-attention once, multi-head attention is used to derive several versions of\nthe attention matrix A which are then compressed into a single matrix, and second, a point-wise\nneural network is used to process the compressed outputs from the multi-head attention.\nWe refer to [17] and [42] for more detail on these network layers.\n3.5 Network Calibration\nThe network’s performance hinges on appropriately calibrating the weights in different layers,\ndenoted as w(k)\nl,j .", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "56", "text": "We refer to [17] and [42] for more detail on these network layers.\n3.5 Network Calibration\nThe network’s performance hinges on appropriately calibrating the weights in different layers,\ndenoted as w(k)\nl,j . In the case of Fully Connected Neural (FCN) layers, these weights manifest as\nmatrices W (k) and a bias term w(k)\n0 for each k ranging from 1 to m. Meanwhile, for Embedding\n(EN) layers, the weights correspond to the coordinates of levels in the new embedding space,\nrepresented as zl(l) for all l in L.\nThe training process involves unconstrained optimization, where a suitable loss functionL(w(k)\nl,j , ·)\nis chosen, and the objective is to find its minimum. The Neural Network (NN) training employs\nthe Back-Propagation (BP) algorithm, wherein weight updates are determined by the gradient\n9", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "57", "text": "of the loss function. The iterative adjustment of weights aims to minimize the error between\nthe network outputs and reference values. The training complexity increases with the number\nof layers and units per layer in the network architecture; these are hyperparameters that should\nbe suitably chosen. Indeed, a too deep NN would lead to overfitting producing a model un-\nable to generalise to new data points, or, alternatively, lead to the vanishing gradient problem,\nwhich prevents the BP algorithm from updating the weights successfully. One remedy for the\noverfitting problem is the application of regularization methods such as dropout. Dropout [37]\nis a stochastic technique that ignores, i.e., sets to zero, some randomly chosen units during the\nnetwork fitting. This is generally achieved by multiplying the output of the different layers by\nindependent realizations of a Bernoulli random variable with parameter p ∈ [0, 1].", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "58", "text": "One remedy for the\noverfitting problem is the application of regularization methods such as dropout. Dropout [37]\nis a stochastic technique that ignores, i.e., sets to zero, some randomly chosen units during the\nnetwork fitting. This is generally achieved by multiplying the output of the different layers by\nindependent realizations of a Bernoulli random variable with parameter p ∈ [0, 1]. Mathemat-\nically, the introduction of the dropout in a FCN layer, for example, in the k-th layer, induces\nthe following structure:\nr(k)\nj ∼ Bernoulli(p)\n˙z(k−1)(x) = r(k) ∗ z(k−1)(x)\nz(k)(x) = ϕ\n\u0010\nwk)\n0 + W (k) ˙z(k−1)(x)\n\u0011\n,\nwhere ∗ denotes the element-wise product and r(k) is a vector of independent Bernoulli random\nvariables, each of which has a probability p of being 1. This mechanism leads to zero the value\nof some elements and encourages more robust learning.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "59", "text": "This mechanism leads to zero the value\nof some elements and encourages more robust learning.\nFor a comprehensive understanding of neural networks and back-propagation, a detailed discus-\nsion can be found in [17].\n4 DeepYC - a neural network model for multiple yield curve\nmodeling and forecasting\nThis section formally outlines the proposed deep learning-based yield curve model, which we\ncall the DeepYC model. We have developed an architecture based on an attention mechanism,\nenabling efficient processing of a (multivariate) time series of yield time series, implicit infor-\nmation selection, and formulation of accurate predictions. Our goal is to derive precise point\nforecasts and effectively measure the uncertainty of the future yield curve. To achieve this, we\nhave designed a model that allows us to jointly estimate a measure of the central tendency of\nthe distribution of future spot rates (mean or median) and two quantiles at a specified tail level,\nforming confidence intervals for the predictions.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "60", "text": "To achieve this, we\nhave designed a model that allows us to jointly estimate a measure of the central tendency of\nthe distribution of future spot rates (mean or median) and two quantiles at a specified tail level,\nforming confidence intervals for the predictions. Denoting as y(i)\nt = ( y(i)\nt (τ))τ ∈M ∈ RM the\nvector (yield curve) containing the spot rates for different maturities related to the curve family\ni, at time t.\nChoosing a look-back period L ∈ N and a confidence level α ∈ (0, 1), we design a model\nthat takes as input the the matrix of yield curves related to the previous L dates denoted as\nY (i)\nt−L,t =\n\u0000\ny(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×M and the label related to the curve family i ∈ I , and produces\nthree output vectorby(i)\nα/2,t,by(i)\nt ,by(i)\n1−α/2,t ∈ RM.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "61", "text": "More specifically, by(i)\nα/2,t is the vector of quantiles\nat levels of confidence α/2, by(i)\nt is the vector of a such measure of the central tendency of the\ndistribution (in what follows, we consider both the mean or the median), and by(i)\n1−α/2,t is the\n10", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "62", "text": "vector of quantiles at level (1 − α/2). The quantile statistics will be calibrated to quantify the\nuncertainty in future yields. We desire to learn the mapping:\nf : R(L+1)×M × I → RM × RM × RM\n\u0000\nY (i)\nt−L,t, i\n\u0001\n7→\n\u0000by(i)\nα/2,t+1,by(i)\nt+1,by(i)\n1−α/2,t+1\n\u0001\n= f\n\u0010\nY (i)\nt−L,t, i\n\u0011\n.\nWe approximate f(·) with a DNN that combines embedding, FCN and attention layers. More\nspecifically, we process the label i using an embedding layer of size qI ∈ N. It is a mapping with\nthe following structure:\neI : I → RqI , i 7→ eI(i) = (eI,1(i), eI,2(i), . . .", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "63", "text": "We approximate f(·) with a DNN that combines embedding, FCN and attention layers. More\nspecifically, we process the label i using an embedding layer of size qI ∈ N. It is a mapping with\nthe following structure:\neI : I → RqI , i 7→ eI(i) = (eI,1(i), eI,2(i), . . . , eI,qI(i))⊤ ,\nwhere eI(i) is a vector that encoded the information related to the family of curves i. It can be\nseen as a new representation of i in a new qI-dimensional real-valued space that is optimal with\nrespect to the response variable. On the other hand, we process the matrix of the past yield\ncurves with three time-distributed FCN layers aiming to derive respectively the query, key and\nvalue vectors for input into the attention component.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "64", "text": ". . , eI,qI(i))⊤ ,\nwhere eI(i) is a vector that encoded the information related to the family of curves i. It can be\nseen as a new representation of i in a new qI-dimensional real-valued space that is optimal with\nrespect to the response variable. On the other hand, we process the matrix of the past yield\ncurves with three time-distributed FCN layers aiming to derive respectively the query, key and\nvalue vectors for input into the attention component. The time-distributed mechanism consists\nof applying the same transformation (the same layer) to each row of the matrixY (i)\nt−L,t containing\nthe yield curves of the different dates. We apply three FCN layers that can be formalised as:\nz(j) : RM → RqA, y(i)\nt 7→ z(j)(y(i)\nt )\nwhere qA is the number of units, and j ∈ {Q, K, V }.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "65", "text": "The time-distributed mechanism consists\nof applying the same transformation (the same layer) to each row of the matrixY (i)\nt−L,t containing\nthe yield curves of the different dates. We apply three FCN layers that can be formalised as:\nz(j) : RM → RqA, y(i)\nt 7→ z(j)(y(i)\nt )\nwhere qA is the number of units, and j ∈ {Q, K, V }.They produce the following matrices:\nQ(i)\nt =\n\u0000\nq(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×qA, q(i)\nt−l = z(Q)(y(i)\nt−l) = ϕ(Q)\u0000\nw(Q)\n0 + W (Q)y(i)\nt−l\n\u0001\n∈ RqA\nK(i)\nt =\n\u0000\nk(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×qA,", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "66", "text": "K, V }.They produce the following matrices:\nQ(i)\nt =\n\u0000\nq(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×qA, q(i)\nt−l = z(Q)(y(i)\nt−l) = ϕ(Q)\u0000\nw(Q)\n0 + W (Q)y(i)\nt−l\n\u0001\n∈ RqA\nK(i)\nt =\n\u0000\nk(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×qA, k(i)\nt−l = z(K)(y(i)\nt−l) = ϕ(K)\u0000\nw(K)\n0 + W (K)y(i)\nt−l\n\u0001\n∈ RqA\nV (i)\nt =\n\u0000\nv(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×qA,", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "67", "text": "k(i)\nt−l = z(K)(y(i)\nt−l) = ϕ(K)\u0000\nw(K)\n0 + W (K)y(i)\nt−l\n\u0001\n∈ RqA\nV (i)\nt =\n\u0000\nv(i)\nt−l\n\u0001\n0≤l≤L ∈ R(L+1)×qA, v(i)\nt−l = z(V )(y(i)\nt−l) = ϕ(V )\u0000\nw(V )\n0 + W (V )y(i)\nt−l\n\u0001\n∈ RqA,\nwhere w(j)\n0 ∈ RqA, W (j) ∈ R\nqA ×M are network parameters, and ϕ(j) : R 7→ R are activation\nfunctions , j ∈ {Q, K, V }.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "68", "text": "v(i)\nt−l = z(V )(y(i)\nt−l) = ϕ(V )\u0000\nw(V )\n0 + W (V )y(i)\nt−l\n\u0001\n∈ RqA,\nwhere w(j)\n0 ∈ RqA, W (j) ∈ R\nqA ×M are network parameters, and ϕ(j) : R 7→ R are activation\nfunctions , j ∈ {Q, K, V }. The three matrices Q(i)\nt , K(i)\nt , V (i)\nt are processed by an attention layer\nthat combine them according to the following transformation:\nX (i)\nt = softmax\n\u0012 Q(i)\nt (K(i)\nt )⊤\n√dk\n\u0013\nV (i)\nt ∈ R(L+1)×qA\nThe output of this layer is arranged in a vector x(i)\nt = vec(X (i)\nt ) that can be interpreted as a\nset of features that summarizes the information contained in the matrix Y (i)\nt−L,t.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "69", "text": "The three matrices Q(i)\nt , K(i)\nt , V (i)\nt are processed by an attention layer\nthat combine them according to the following transformation:\nX (i)\nt = softmax\n\u0012 Q(i)\nt (K(i)\nt )⊤\n√dk\n\u0013\nV (i)\nt ∈ R(L+1)×qA\nThe output of this layer is arranged in a vector x(i)\nt = vec(X (i)\nt ) that can be interpreted as a\nset of features that summarizes the information contained in the matrix Y (i)\nt−L,t. We remark that\nother deep learning models could be used to derive the vector of features from the matrix of\nthe past data; here, we use the attention mechanism, having found that this produces the best\nout-of-sample forecasting performance.\nFinally, we apply three different |M|-dimensional FCN layers to the set of features x(i)\nt for\nderiving the predictions related to the three vectors of output statistics of interest.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "70", "text": "We remark that\nother deep learning models could be used to derive the vector of features from the matrix of\nthe past data; here, we use the attention mechanism, having found that this produces the best\nout-of-sample forecasting performance.\nFinally, we apply three different |M|-dimensional FCN layers to the set of features x(i)\nt for\nderiving the predictions related to the three vectors of output statistics of interest. The first\nlayer aims to derive the lower quantiles of the yields, the second one aims to compute the mean\n(or the median), while the third one is related to the upper quantiles. Each unit of these layers\nis dedicated to a specific maturity and we have in each layer as many units as the maturities\nconsidered. Before inputting the outputs of the attention layer x(i)\nt to these FCNs, we apply\n11", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "71", "text": "dropout to x(i)\nt for regularization and reducing overfitting. In this setting, the predictions are\nobtained according to the following set of equations:\nr(att)\nj ∼ Bernoulli(p(att)\n1 )\n˙x(i)\nt = r(att) ∗ x(i)\nt\nby(i)\nt+1 = g\n\u0012\nbc + Uce(i) + Wc ˙x(i)\nt\n\u0013\n(4.1)\nby(i)\nα/2,t+1 =by(i)\nt+1 − ϕ+\n\u0012\nblb + Ulbe(i) + Wlb ˙x(i)\nt\n\u0013\n(4.2)\nby(i)\n1−α/2,t+1 =by(i)\nt+1 + ϕ+\n\u0012\nbub + Uube(i) + Wub ˙x(i)\nt\n\u0013\n(4.3)\nwhere p(att)\n1 ∈ [0, 1] is the dropout rate, g : R → R and ϕ+ : R →]0,", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "72", "text": "t+1 =by(i)\nt+1 − ϕ+\n\u0012\nblb + Ulbe(i) + Wlb ˙x(i)\nt\n\u0013\n(4.2)\nby(i)\n1−α/2,t+1 =by(i)\nt+1 + ϕ+\n\u0012\nbub + Uube(i) + Wub ˙x(i)\nt\n\u0013\n(4.3)\nwhere p(att)\n1 ∈ [0, 1] is the dropout rate, g : R → R and ϕ+ : R →]0, +∞) are strictly monotone\nfunctions, and bj, Uj, Wj, j ∈ {c, lb, ub} are network parameters.\nLooking at this set of equations, some remarks can be made:\n(1) The model presents some connections with the affine models1 discussed in [31].", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "73", "text": "3)\nwhere p(att)\n1 ∈ [0, 1] is the dropout rate, g : R → R and ϕ+ : R →]0, +∞) are strictly monotone\nfunctions, and bj, Uj, Wj, j ∈ {c, lb, ub} are network parameters.\nLooking at this set of equations, some remarks can be made:\n(1) The model presents some connections with the affine models1 discussed in [31]. Considering\na single maturity τ, equation (4.1) can be formulated as follows:\ng(−1)\u0000by(i)\nt+1(τ)\n\u0001\n= bc +\nD\nuc,τ , e(i)\nE\n+\nD\nwc,τ , ˙x(i)\nt\nE\n.\nIndeed, it has the constant-plus-linear structure and depends on the vector of variables\nx(i)\nt derived by the past observed data. Furthermore, the following additional arguments\ncan be provided:\n– bc can be interpreted as a sort of global intercept;\n–", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "74", "text": "Considering\na single maturity τ, equation (4.1) can be formulated as follows:\ng(−1)\u0000by(i)\nt+1(τ)\n\u0001\n= bc +\nD\nuc,τ , e(i)\nE\n+\nD\nwc,τ , ˙x(i)\nt\nE\n.\nIndeed, it has the constant-plus-linear structure and depends on the vector of variables\nx(i)\nt derived by the past observed data. Furthermore, the following additional arguments\ncan be provided:\n– bc can be interpreted as a sort of global intercept;\n–\n\nuc,τ , e(i)\u000b\nis an intercept correction related to the curve family i and maturity τ;\n– wc,τ is the vector of maturity-specific weights associated with x(i)\nt . These coefficients\nare shared among all the curve families since these do not depend on i.\n(2) The formulation we propose avoids potential quantile crossing, which refers to the scenario\nin which the estimated quantiles of a probability distribution do not respect the expected\norder.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "75", "text": "Furthermore, the following additional arguments\ncan be provided:\n– bc can be interpreted as a sort of global intercept;\n–\n\nuc,τ , e(i)\u000b\nis an intercept correction related to the curve family i and maturity τ;\n– wc,τ is the vector of maturity-specific weights associated with x(i)\nt . These coefficients\nare shared among all the curve families since these do not depend on i.\n(2) The formulation we propose avoids potential quantile crossing, which refers to the scenario\nin which the estimated quantiles of a probability distribution do not respect the expected\norder. Inaccurate or inconsistent quantile estimates have the potential to affect the relia-\nbility and interpretability of predictions from the model. Indeed, the use of the activation\nfunction ϕ+(·) that assumes only positive values ensures that:\nby(i)\nα/2,t+1 <by(i)\nt+1 <by(i)\nα/2,t+1.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "76", "text": "Inaccurate or inconsistent quantile estimates have the potential to affect the relia-\nbility and interpretability of predictions from the model. Indeed, the use of the activation\nfunction ϕ+(·) that assumes only positive values ensures that:\nby(i)\nα/2,t+1 <by(i)\nt+1 <by(i)\nα/2,t+1.\n(3) Rewriting equations (4.2) for a single maturity, we have:\nϕ−1\n\u0012\nby(i)\nt+1(τ) −by(i)\nα/2,t+1(τ)\n\u0013\n= blb +\nD\nulb,τ , e(i)\nE\n+\nD\nwlb,τ , ˙x(i)\nt\nE\nemphasizing that we model, on the ϕ(−1) scale, the difference between the central measure\nand lower quantile at a given maturity τ is an affine model. Similar comments can be\nmade for the difference between the upper quantile and the central measure.\n12", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "77", "text": "Figure 1: Diagram of the feature processing components of the DeepYC model. A matrix of\nspot rates is processed by three FCN layers in a time-distributed manner, to derive the key,\nquery and value matrices which are then input into a self-attention operation.\n13", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "78", "text": "111\n111\nFigure 2: Diagram of the output components of the DeepYC model. The matrix of features\nproduced by the first part of the model are flattened into a vector and then dropout is applied.\nWe add a categorical embedding to this vector, and, finally, then the best-estimate and quantile\npredictions are produced.\n14", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "79", "text": "We illustrate the DeepYC model in Figures 1 and 2.\nThe calibration of the multi-output network is carried out accordingly, with a loss function\nspecifically designed for our aim. It is the sum of three components:\nLα,γ(θ) = L(1)\nα/2(θ) + L(2)\nγ (θ) + L(3)\n1−α/2(θ)\n=\nX\ni,t,τ\nℓα/2(y(i)\nt (τ) − ˆy(i)\nα/2,t(τ)) +\nX\ni,t,τ\nhγ(y(i)\nt (τ) − ˆy(i)\nt (τ)) +\nX\ni,t,τ\nℓ1−α/2(y(i)\nt (τ) − ˆy(i)\n1−α/2,t(τ))\n(4.4)\nwhere ℓα(u), α ∈ [0,", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "80", "text": "t,τ\nℓα/2(y(i)\nt (τ) − ˆy(i)\nα/2,t(τ)) +\nX\ni,t,τ\nhγ(y(i)\nt (τ) − ˆy(i)\nt (τ)) +\nX\ni,t,τ\nℓ1−α/2(y(i)\nt (τ) − ˆy(i)\n1−α/2,t(τ))\n(4.4)\nwhere ℓα(u), α ∈ [0, 1] is the pinball function:\nℓα(u) =\n(\n(1 − α)|u| u ≤ 0\nα|u| u > 0,\nand hγ(u), γ ∈ {1, 2} is:\nhγ(u) =\n(\n|u| γ = 1\nu2 γ = 2,\nWe emphasize that the first term of the loss function is the pinball function with parameter\nα/2 associated with the estimation of the lower quantile.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "81", "text": "t(τ))\n(4.4)\nwhere ℓα(u), α ∈ [0, 1] is the pinball function:\nℓα(u) =\n(\n(1 − α)|u| u ≤ 0\nα|u| u > 0,\nand hγ(u), γ ∈ {1, 2} is:\nhγ(u) =\n(\n|u| γ = 1\nu2 γ = 2,\nWe emphasize that the first term of the loss function is the pinball function with parameter\nα/2 associated with the estimation of the lower quantile. The second term represents a generic\nfunction linked to the estimation of the central tendency, while the last term is the pinball\nfunction with parameters α/2 associated with the estimation of the upper quantile.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "82", "text": "and hγ(u), γ ∈ {1, 2} is:\nhγ(u) =\n(\n|u| γ = 1\nu2 γ = 2,\nWe emphasize that the first term of the loss function is the pinball function with parameter\nα/2 associated with the estimation of the lower quantile. The second term represents a generic\nfunction linked to the estimation of the central tendency, while the last term is the pinball\nfunction with parameters α/2 associated with the estimation of the upper quantile.\nRegarding the function hγ(·), it is noteworthy that setting γ = 2 means that we use the Mean\nSquared Error (MSE), indicating that we are modeling the mean of the yields; alternatively,\nsetting γ = 1, hγ is the the Mean Absolute Error (MAE), signifying that we are modeling the\nmedian.\n5 Numerical Experiments\nWe present some numerical experiments conducted on data provided by the European Insurance\nand Occupational Pensions Authority (EIOPA).", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "83", "text": "Regarding the function hγ(·), it is noteworthy that setting γ = 2 means that we use the Mean\nSquared Error (MSE), indicating that we are modeling the mean of the yields; alternatively,\nsetting γ = 1, hγ is the the Mean Absolute Error (MAE), signifying that we are modeling the\nmedian.\n5 Numerical Experiments\nWe present some numerical experiments conducted on data provided by the European Insurance\nand Occupational Pensions Authority (EIOPA). The authority publishes risk-free interest rate\nterm structures derived from government bonds of various countries; these are published on a\nmonthly basis as spot curves, which are the default option for use in the Solvency II regime for\ndiscount rates. We define I as the set of all available countries andM = {τ ∈ N : τ ≤ 150}. Our\nsample data covers the period from December 2015 to December 2021, and Figure 9 provides a\ngraphical representation of the EIOPA data.\nSelecting an observation time t0, we partition the full dataset into two parts.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "84", "text": "The authority publishes risk-free interest rate\nterm structures derived from government bonds of various countries; these are published on a\nmonthly basis as spot curves, which are the default option for use in the Solvency II regime for\ndiscount rates. We define I as the set of all available countries andM = {τ ∈ N : τ ≤ 150}. Our\nsample data covers the period from December 2015 to December 2021, and Figure 9 provides a\ngraphical representation of the EIOPA data.\nSelecting an observation time t0, we partition the full dataset into two parts. The first, contain-\ning yields before time t0 (referred to as the learning sample), is used for model calibration. The\nsecond, encompassing data after time t0 (referred to as the testing sample ), is employed to eval-\nuate the out-of-sample accuracy of the models. Interval forecasts are constructed by considering\nthe case in which we desire a coverage probability α = 0.95.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "85", "text": "Selecting an observation time t0, we partition the full dataset into two parts. The first, contain-\ning yields before time t0 (referred to as the learning sample), is used for model calibration. The\nsecond, encompassing data after time t0 (referred to as the testing sample ), is employed to eval-\nuate the out-of-sample accuracy of the models. Interval forecasts are constructed by considering\nthe case in which we desire a coverage probability α = 0.95.\nTo benchmark our model, we consider the Dynamic NS proposed in [9] and its related Svensson\n(NSS) extension. We examine both cases where the latent factors follow individual AR(1) models\n1The term “affine term structure model” is used in different ways by the literature, we refer to the definition\ngiven in Chapter 12 of [31].\n15", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "86", "text": "Model MSE MAE PICP MPIW\nNS AR 0.7433 0.4496 0.9984 0.0540\nNS VAR 0.4977 0.3492 0.7288 0.0080\nNSS AR 0.5379 0.3709 0.9987 0.4253\nNSS VAR 0.4626 0.3226 0.7462 0.0307\nTable 1: Performance of the NS and NSS models in terms of MSE, MAE, PICP and MPIW;\nThe MSE values are scaled by a factor of 10 5, while the MAE values are scaled by a factor of\n102.\nand the case of a single multivariate VAR(1) model, denoted respectively as NS AR (NSS AR)\nand NS VAR (NSS VAR).\nSince our focus is on measuring forecasting accuracy in terms of both point and interval forecasts,\nwe employ several metrics to compare the models.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "87", "text": "Since our focus is on measuring forecasting accuracy in terms of both point and interval forecasts,\nwe employ several metrics to compare the models. Concerning point forecast accuracy, we use\nthe global (i.e., evaluated for all countries in the dataset) Mean Squared Error (MSE) and the\nMean Absolute Error (MAE) defined as follows:\nMSE = 1\nn\nX\ni∈I\nX\nt∈T\nX\nτ ∈M\n(y(i)\nt (τ) − ˆy(i)\nt (τ))2,\nMAE = 1\nn\nX\ni∈I\nX\nt∈T\nX\nτ ∈M\n|y(i)\nt (τ) − ˆy(i)\nt (τ)|,\nwhere n ∈ N is the number of instances. We emphasize that MSE is based on the l2-norm of\nthe errors and penalizes a larger deviation from the observed values with respect to the MAE\nwhich is based on the l1-norm of the errors.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "88", "text": "We emphasize that MSE is based on the l2-norm of\nthe errors and penalizes a larger deviation from the observed values with respect to the MAE\nwhich is based on the l1-norm of the errors. For measuring the interval prediction accuracy, we\nconsider the Prediction Interval Coverage Probability (PICP):\nPICP = 1\nn\nX\ni∈I\nX\nt∈T\nX\nτ ∈M\n1 {y(i)\nt (τ ) ∈ [ˆy(i)\nt,LB (τ ), ˆy(i)\nt,U B(τ )]} (5.1)\nFurthermore, we also analyse the Mean Prediction Interval Width (MPIW) to take into account\nthe width of the confidence interval:\nMPIW = 1\nn\nX\ni∈I\nX\nt∈T\nX\nτ ∈M\n\u0000\nˆy(i)\nt,U B(τ) − ˆy(i)\nt,LB(τ)\n\u0001\n.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "89", "text": "(5.2)\nIntuitively, a good model should provide a PICP close to the value of α, indicating that the\nmodel is well calibrated, while having as small an MPIW, as possible. Table 1 presents the\nperformance related to the four measures considered for the NS and NSS benchmark models.\nRegarding the accuracy of point forecasts, it is worth noting that employing a VAR(1) model\ninstead of independent AR(1) models for modeling the latent factors dynamics enhances the\nperformance of both the NS and NSS models. This improvement is evident in terms of both\nMean Squared Error (MSE) and Mean Absolute Error (MAE). Furthermore, it is noteworthy\nthat NSS models consistently demonstrate superior accuracy compared to NS models. Turning\nour attention to interval forecasts, it becomes apparent that models incorporating AR processes\ntend to exhibit over coverage, as indicated by excessively high Prediction Interval Coverage\nProbability (PICP) and impractically large interval widths providing very limited information\n16", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "90", "text": "content. In contrast, versions based on VAR models for NS and NSS tend to yield more reason-\nable interval widths. However, given their relatively low coverage probability, they fall short of\nadequately capturing the uncertainty in future yields. In summary, we designate the NSS VAR\nmodel as the optimal choice, as it yields the lowest MSE and MAE, along with the highest PICP\nand a reasonable MPIW. Consequently, we will employ this model for subsequent comparisons\nthroughout the remainder of the paper.\nNow, we can focus on the YC ATT model. We examine two variants distinguished by the gamma\nparameter in the loss function employed for calibration. The first variant is calibrated by setting\nγ = 1 (denoted as YC ATTγ=1) and, in that case, the second component of the loss function is\nthe MAE. On the other hand, the second variant is calibrated with γ = 2 and the component\nof the loss related to the central tendency of the distribution then uses the MSE.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "91", "text": "Now, we can focus on the YC ATT model. We examine two variants distinguished by the gamma\nparameter in the loss function employed for calibration. The first variant is calibrated by setting\nγ = 1 (denoted as YC ATTγ=1) and, in that case, the second component of the loss function is\nthe MAE. On the other hand, the second variant is calibrated with γ = 2 and the component\nof the loss related to the central tendency of the distribution then uses the MSE. Regarding the\nother hyperparameters, we have configured the number of units in the dense layers comprising\nthe attention component of the model as qA = 8, while the dropout rate is set p(att)\n1 = 0.5. As\npreviously indicated, the part of the network architecture that processes the past yields can be\nconstructed using various neural network blocks. The attention layer, discussed earlier, is just\none of the available choices. We further explore the utilization of other well-established deep\nlearning models that have demonstrated success in modeling sequential data.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "92", "text": "As\npreviously indicated, the part of the network architecture that processes the past yields can be\nconstructed using various neural network blocks. The attention layer, discussed earlier, is just\none of the available choices. We further explore the utilization of other well-established deep\nlearning models that have demonstrated success in modeling sequential data. Specifically, we\ninvestigate other 3 variants:\n• YC LSTM: based on the Long Short Term Memory network (LSTM) of [21] that is a\npopular kind of RNN;\n• A simplification of the YC ATT model, that removes the attention mechniasm and relies\nonly on processing the yield curves using time-distirubted FCNs; since these are also called\none-dimensional convolutional neural networks, we call this variant YC CONV;\n• YC TRANS which is a more complex Transformer based model [40], which adds extra\nFCNs to the YC ATT model.\nIn order to make the comparison fair, also for these variants we set the number of units in the\nlayers equal to 8.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "93", "text": "In order to make the comparison fair, also for these variants we set the number of units in the\nlayers equal to 8. Furthermore, for each one of these architectures we respectively test the\nvariants with γ = 1 and γ = 2.\nTable 2 presents the performance metrics, including Mean Squared Error (MSE), Mean Absolute\nError (MAE), Prediction Interval Coverage Probability (PICP), and Mean Prediction Interval\nWidth (MPIW), for various deep learning models.\nTo account for randomness in batch sampling and the random initial parameters for each network\nfor the optimization, multiple training attempts were conducted, and the results represent the\naverage performance over 1 training attempts. The boxplots related to the 10 training attempts\nare shown in Figure 10 (Appendix A). Additionally, ensemble predictions were generated by\naveraging the forecasts from the ten trained models. We remark that the average MPIW model\non the different training attempts and the MPIW related to the ensemble predictions coincide.\nThe proof of this statement is reported in the Appendix.\nSeveral noteworthy findings emerge.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "94", "text": "To account for randomness in batch sampling and the random initial parameters for each network\nfor the optimization, multiple training attempts were conducted, and the results represent the\naverage performance over 1 training attempts. The boxplots related to the 10 training attempts\nare shown in Figure 10 (Appendix A). Additionally, ensemble predictions were generated by\naveraging the forecasts from the ten trained models. We remark that the average MPIW model\non the different training attempts and the MPIW related to the ensemble predictions coincide.\nThe proof of this statement is reported in the Appendix.\nSeveral noteworthy findings emerge. Firstly, the consistent adoption of ensemble mechanisms\nleads to superior performance compared to the average performance across individual models.\nThis trend holds across all examined models and metrics, aligning with prevailing findings in the\nliterature on predictive modeling with deep learning. Moreover, a notable trend is highlighted:\n17", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "95", "text": "MSE MAE PICP MPIW\nModel average ensemble average ensemble average ensemble average ensemble\nYC ATTγ=1 0.2947 0.2887 0.2667 0.2616 0.9154 0.9191 0.0106 0.0106\nYC ATTγ=2 0.3663 0.3638 0.3463 0.3451 0.8528 0.8573 0.0105 0.0105\nYC CONVγ=1 0.3778 0.3642 0.2975 0.2850 0.9035 0.9235 0.0115 0.115\nYC CONVγ=2 0.4258 0.4244 0.3890 0.3884 0.8509 0.8530 0.0110 0.0110\nYC LSTMγ=1 0.4272 0.4111 0.3164 0.2970 0.7757 0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "96", "text": "3778 0.3642 0.2975 0.2850 0.9035 0.9235 0.0115 0.115\nYC CONVγ=2 0.4258 0.4244 0.3890 0.3884 0.8509 0.8530 0.0110 0.0110\nYC LSTMγ=1 0.4272 0.4111 0.3164 0.2970 0.7757 0.8147 0.0093 0.0093\nYC LSTMγ=2 0.3898 0.3697 0.3352 0.3198 0.6911 0.7081 0.0084 0.0084\nYC TRANγ=1 0.4308 0.4167 0.3313 0.3168 0.8371 0.8645 0.0113 0.0113\nYC TRANSγ=2 0.4232 0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "97", "text": "2970 0.7757 0.8147 0.0093 0.0093\nYC LSTMγ=2 0.3898 0.3697 0.3352 0.3198 0.6911 0.7081 0.0084 0.0084\nYC TRANγ=1 0.4308 0.4167 0.3313 0.3168 0.8371 0.8645 0.0113 0.0113\nYC TRANSγ=2 0.4232 0.4124 0.4042 0.3987 0.5771 0.5760 0.0091 0.0091\nTable 2: Out-of-sample performance of the different deep learning models in terms of MSE,\nMAE, PICP and MPIW; the MSE values are scaled by a factor of 10 5, while the MAE values\nare scaled by a factor of 10 2.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "98", "text": "8371 0.8645 0.0113 0.0113\nYC TRANSγ=2 0.4232 0.4124 0.4042 0.3987 0.5771 0.5760 0.0091 0.0091\nTable 2: Out-of-sample performance of the different deep learning models in terms of MSE,\nMAE, PICP and MPIW; the MSE values are scaled by a factor of 10 5, while the MAE values\nare scaled by a factor of 10 2. Bold indicates the smallest value, or, for the PICP, the value\nclosest to α = 0.95.\nmodels configured with γ = 1 consistently outperform their counterparts with γ = 2 in both\npoint forecasts and interval forecast accuracy. This outcome can be attributed to the robust\nnature of MAE as a measure, providing increased resilience to outliers and contributing to a\nmore stable model calibration. Suprisingly, this is even the case when evaluating the models\nusing the MSE metric.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "99", "text": "while the MAE values\nare scaled by a factor of 10 2. Bold indicates the smallest value, or, for the PICP, the value\nclosest to α = 0.95.\nmodels configured with γ = 1 consistently outperform their counterparts with γ = 2 in both\npoint forecasts and interval forecast accuracy. This outcome can be attributed to the robust\nnature of MAE as a measure, providing increased resilience to outliers and contributing to a\nmore stable model calibration. Suprisingly, this is even the case when evaluating the models\nusing the MSE metric. Finally, among the deep learning models, the YC ATT model with\nγ = 1 demonstrates the best performance, suggesting that this architecture is particularly well-\nsuited for the regression task at hand. Also notable is that the ensemble predictions of the\nYC ATT model with γ = 1 are almost the best calibrated, as measured by the PICP metric,\nwhile the bounds are relatively narrow, as measured by the MPIW metric.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "100", "text": "Suprisingly, this is even the case when evaluating the models\nusing the MSE metric. Finally, among the deep learning models, the YC ATT model with\nγ = 1 demonstrates the best performance, suggesting that this architecture is particularly well-\nsuited for the regression task at hand. Also notable is that the ensemble predictions of the\nYC ATT model with γ = 1 are almost the best calibrated, as measured by the PICP metric,\nwhile the bounds are relatively narrow, as measured by the MPIW metric. Nonetheless, the\nbest performance on the PICP metric on a standalone basis is the YC CONV with γ = 1.\nRegarding ensemble predictions, it is noteworthy that the average MPIW model across various\ntraining attempts coincides with the MPIW associated with ensemble predictions. The proof of\nthis assertion can be found in the Appendix. In summary, evaluating the models on all of the\nmetrics, the YC ATT performs the best overall.\nFigure 5 graphically compares the point and interval forecasts generated by the NSS VAR and\nYC ATT models for yield curves across various countries.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "101", "text": "Nonetheless, the\nbest performance on the PICP metric on a standalone basis is the YC CONV with γ = 1.\nRegarding ensemble predictions, it is noteworthy that the average MPIW model across various\ntraining attempts coincides with the MPIW associated with ensemble predictions. The proof of\nthis assertion can be found in the Appendix. In summary, evaluating the models on all of the\nmetrics, the YC ATT performs the best overall.\nFigure 5 graphically compares the point and interval forecasts generated by the NSS VAR and\nYC ATT models for yield curves across various countries. The figure refers to the central date\nwithin the forecasting horizon, specifically June 2021, as the forecasting period spans monthly\nobservations from January to December 2021. The figure shows the actual observed yield curve,\nand an out-of-sample forecast of the best-estimate and quantiles using the previous 10 months of\ndata; model parameters are those calibrated using data up to the end of 2020. Upon examination,\nit becomes evident that the NSS VAR model falls short in accurately depicting the shape of\nyield curves in certain countries.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "102", "text": "The figure refers to the central date\nwithin the forecasting horizon, specifically June 2021, as the forecasting period spans monthly\nobservations from January to December 2021. The figure shows the actual observed yield curve,\nand an out-of-sample forecast of the best-estimate and quantiles using the previous 10 months of\ndata; model parameters are those calibrated using data up to the end of 2020. Upon examination,\nit becomes evident that the NSS VAR model falls short in accurately depicting the shape of\nyield curves in certain countries. Notably, the realized yields deviate significantly from the\nprojected forecasts and extend beyond the confidence intervals. This discrepancy is particularly\npronounced in the cases of Brazil, Colombia, Mexico, India, Russia, South Africa, and others. In\ncontrast, the YC ATT model exhibits more flexibility and demonstrates the ability of effectively\ncapturing the uncertainties associated with future yields.\nFigure 4 offers a more in-depth analysis of the performance exhibited by the YC ATT and\nNSS VAR models across various countries of the EIOPA yield curves.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "103", "text": "Notably, the realized yields deviate significantly from the\nprojected forecasts and extend beyond the confidence intervals. This discrepancy is particularly\npronounced in the cases of Brazil, Colombia, Mexico, India, Russia, South Africa, and others. In\ncontrast, the YC ATT model exhibits more flexibility and demonstrates the ability of effectively\ncapturing the uncertainties associated with future yields.\nFigure 4 offers a more in-depth analysis of the performance exhibited by the YC ATT and\nNSS VAR models across various countries of the EIOPA yield curves. It illustrates the MSE,\nMAE, and PICP produced by the different models for the different yield curve families. From the\nstandpoint of point forecasts, it is noted that in certain instances, the YC ATT and NSS VAR\n18", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "104", "text": "Switzerland Taiwan Thailand Turkey United.Kingdom United.States\nPoland Romania Russia Singapore South.Africa South.Korea Sweden\nIndia Japan Liechtenstein Malaysia Mexico New.Zealand Norway\nCroatia Czech.Republic Denmark Euro Hong.Kong Hungary Iceland\nAustralia Brazil Bulgaria Canada Chile China Colombia\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0.02\n0.04\n0.06\n0.08\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "105", "text": "02\n0.04\n0.06\n0.08\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n0.03\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.02\n0.04\n0.06\n0.08\n0.10\n0.00\n0.01\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "106", "text": "03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n0.03\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.02\n0.04\n0.06\n0.08\n0.10\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.03\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.05\n0.10\n0.15\n0.20\n0.25\n−0.02\n−0.01\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "107", "text": "06\n0.08\n0.10\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.03\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.05\n0.10\n0.15\n0.20\n0.25\n−0.02\n−0.01\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.04\n0.06\n0.08\n0.00\n0.01\n0.02\n0.03\n0.03\n0.06\n0.09\n0.00\n0.01\n0.02\n0.03\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "108", "text": "20\n0.25\n−0.02\n−0.01\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.04\n0.06\n0.08\n0.00\n0.01\n0.02\n0.03\n0.03\n0.06\n0.09\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.03\n0.05\n0.07\n0.00\n0.01\n0.02\n0.03\n−0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "109", "text": "00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.03\n0.05\n0.07\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\nMaturity\nvalue\nModel\nactual data\nNS_VAR\nYC_ATT\nFigure 3: Point and interval forecasts for EIOPA yield curves generated by the NSS VAR and YC ATT models as of June 2021, the central\ndate of the forecasting period.\n19", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "110", "text": "●● ● ● ● ● ● ● ● ● ● ● ● ● ●●\n●\n●\n●\n●\n●\n● ● ● ● ●\n● ● ● ● ● ●\n●\n●●● ● ● ● ● ● ● ●\n● ●\n●\n● ● ●●\n●\n●\n●\n●\n●\n● ● ● ●\n● ●\n●\n●\n● ● ●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n● ● ●\n● ● ●●\n●\n●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n● ●\n● ● ●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "111", "text": "● ●\n● ● ●\n● ● ●●\n●\n●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n● ●\n● ● ● ●●\n●\n●\n●\n●\n●\n● ● ●\n● ●\n●\n● ● ● ●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "112", "text": "●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n● ●\n● ● ● ●●\n●\n●\n●\n●\n●\n● ● ●\n● ●\n●\n● ● ● ●\n●\n●\n●\n●● ●\n●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ● ●\n●\n●\nPICP\nMAE\nMSE\nAustralia\nBrazil\nBulgaria\nCanada\nChile\nChina\nColombia\nCroatia\nCzech.Republic\nDenmark\nEuro\nHong.Kong\nHungary\nIceland\nIndia\nJapan\nLiechtenstein\nMalaysia\nMexico\nNew.Zealand\nNorway\nPoland\nRomania\nRussia\nSingapore\nSouth.Africa\nSouth.Korea\nSweden\nSwitzerland\nTaiwan\nThailand\nTurkey\nUnited.Kingdom\nUnited.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "113", "text": "● ●\n●\n●\nPICP\nMAE\nMSE\nAustralia\nBrazil\nBulgaria\nCanada\nChile\nChina\nColombia\nCroatia\nCzech.Republic\nDenmark\nEuro\nHong.Kong\nHungary\nIceland\nIndia\nJapan\nLiechtenstein\nMalaysia\nMexico\nNew.Zealand\nNorway\nPoland\nRomania\nRussia\nSingapore\nSouth.Africa\nSouth.Korea\nSweden\nSwitzerland\nTaiwan\nThailand\nTurkey\nUnited.Kingdom\nUnited.States\n0\n2\n4\n6\n0\n50\n100\n150\n0.25\n0.50\n0.75\n1.00\nCountry\nvalue\nModel\n●\n●\nYC_ATT\nNSS_VAR\nFigure 4: MSE, MAE, and PICP obtained by the YC ATT and NSS VAR models in the different\ncountries.\nmodels produce comparable results.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "114", "text": "Africa\nSouth.Korea\nSweden\nSwitzerland\nTaiwan\nThailand\nTurkey\nUnited.Kingdom\nUnited.States\n0\n2\n4\n6\n0\n50\n100\n150\n0.25\n0.50\n0.75\n1.00\nCountry\nvalue\nModel\n●\n●\nYC_ATT\nNSS_VAR\nFigure 4: MSE, MAE, and PICP obtained by the YC ATT and NSS VAR models in the different\ncountries.\nmodels produce comparable results. However, for specific cases, the YC ATT model demon-\nstrates a notable enhancement, particularly evident in developing countries such as Brazil, Chile,\nMalaysia, Mexico, South Africa, and Turkey. This finding suggests that yields shows that when\nthe yield curve evolution can be adequately described by the NSS VAR model, our YC ATT\ntends to replicate the same point predictions. On the other hand, for the yield curve families for\nwhich the NSS VAR model model is not optimal, the YC ATT improve the results.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "115", "text": "models produce comparable results. However, for specific cases, the YC ATT model demon-\nstrates a notable enhancement, particularly evident in developing countries such as Brazil, Chile,\nMalaysia, Mexico, South Africa, and Turkey. This finding suggests that yields shows that when\nthe yield curve evolution can be adequately described by the NSS VAR model, our YC ATT\ntends to replicate the same point predictions. On the other hand, for the yield curve families for\nwhich the NSS VAR model model is not optimal, the YC ATT improve the results. In terms\nof interval forecasts, there is a noticeable improvement attributable to the YC ATT model,\nimpacting all the EIOPA yield curves under consideration.\nTo gain insights into the mechanism underlying the YC ATT model, we investigate the features\nthat the model extracts from the input data and that are used by the output layers to derive\nkey statistics. We consider the feature vector ( e(i), x(i)\nt ) ∈ RqI +qA×M, and conduct Principal\nComponent Analysis (PCA) to reduce the dimensionality.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "116", "text": "In terms\nof interval forecasts, there is a noticeable improvement attributable to the YC ATT model,\nimpacting all the EIOPA yield curves under consideration.\nTo gain insights into the mechanism underlying the YC ATT model, we investigate the features\nthat the model extracts from the input data and that are used by the output layers to derive\nkey statistics. We consider the feature vector ( e(i), x(i)\nt ) ∈ RqI +qA×M, and conduct Principal\nComponent Analysis (PCA) to reduce the dimensionality. We extract the first four Principal\nComponents (PCs) which explain the 97% of the variability of ( e(i), x(i)\nt ) such that we have a\nmapping with the structure RqI +qA×M 7→ R4. To assess the similarity of information contained\nin the four PCs with theβ(i)\nt factors of the NSS model, we compute the linear correlation between\nthese two sets of features. Figure 4 visually represents the average absolute value of the Pearson\nlinear correlation for each yield curve family.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "117", "text": "We extract the first four Principal\nComponents (PCs) which explain the 97% of the variability of ( e(i), x(i)\nt ) such that we have a\nmapping with the structure RqI +qA×M 7→ R4. To assess the similarity of information contained\nin the four PCs with theβ(i)\nt factors of the NSS model, we compute the linear correlation between\nthese two sets of features. Figure 4 visually represents the average absolute value of the Pearson\nlinear correlation for each yield curve family. Notably, we observe that very high correlations\nare detected in some cases, while low correlations are obtained in the others. Examining this\nfigure in conjunction with Figure 3, we note that in families exhibiting similar performances,\nsuch as Euro, Bulgaria, Denmark, and Iceland, the PC components are high correlated with the\nNSS latent factors. Conversely, instances of notable improvements by the YC ATT model, as\nseen in Mexico, Turkey, Malaysia, and Brazil, are accompanied by smaller correlations between\n20", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "118", "text": "Switzerland Taiwan Thailand Turkey United.Kingdom United.States\nPoland Romania Russia Singapore South.Africa South.Korea Sweden\nIndia Japan Liechtenstein Malaysia Mexico New.Zealand Norway\nCroatia Czech.Republic Denmark Euro Hong.Kong Hungary Iceland\nAustralia Brazil Bulgaria Canada Chile China Colombia\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nbeta_0\nbeta_1\nbeta_2\nbeta_3\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "119", "text": "00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\n0.00\n0.25\n0.50\n0.75\nparameter\naverage\nparameter\nbeta_0\nbeta_1\nbeta_2\nbeta_3\nFigure 5: Point and interval forecasts for the yield curves of various families are generated by the NSS VAR and YC ATT models for the\nEIOPA data in the central date of forecasting timeframe.\n21", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "120", "text": "the PC components from the output of the attention layer and the beta parameters of NSS.\nIn essence, this figure confirms that in cases where the yield curves follow a process adequately\ndescribed by the NSS models, the YC ATT model replicates this by extracting variables highly\ncorrelated with beta. However, when this is not the case, and the yields present more complex\npatterns, our attention model derives features that deviate from the NNS model, resulting in\nbetter outcomes.\n6 Extensions and variants of the YC ATT model\nIn this section, we explore potential extensions and variants of the YC ATT, considering some\nmodifications that aim to enhance the modeling of yield curves and their associated uncertainties.\n6.1 Deep Ensemble\nAn alternative approach for modeling uncertainty in future yields is the Deep Ensemble (DE)\nmethod discussed in [25]. In contrast to the quantile regression-based YC ATT, this method\nrelies on distributional assumptions for the response.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "121", "text": "6 Extensions and variants of the YC ATT model\nIn this section, we explore potential extensions and variants of the YC ATT, considering some\nmodifications that aim to enhance the modeling of yield curves and their associated uncertainties.\n6.1 Deep Ensemble\nAn alternative approach for modeling uncertainty in future yields is the Deep Ensemble (DE)\nmethod discussed in [25]. In contrast to the quantile regression-based YC ATT, this method\nrelies on distributional assumptions for the response. The idea consists of formulating a het-\neroscedastic Gaussian regression model that provides joint estimates for both the mean and\nvariance of the yields, denoted as ( y(i)\nt (τ), (σ(i)\nt (τ))2). This technique not only facilitates the\nextraction of additional insights into future yields but also accommodates heteroscedasticity in\nthe modeling process; this is different from the networks calibrated in the previous section which\nare equivalent to assuming that the responses follow a homoscedastic Laplace or Gaussian dis-\ntribution for choices of γ ∈ 0, 1 respectively.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "122", "text": "This technique not only facilitates the\nextraction of additional insights into future yields but also accommodates heteroscedasticity in\nthe modeling process; this is different from the networks calibrated in the previous section which\nare equivalent to assuming that the responses follow a homoscedastic Laplace or Gaussian dis-\ntribution for choices of γ ∈ 0, 1 respectively. Furthermore, confidence intervals can be derived\nusing these estimates. In this vein, we design a network architecture with two output layers\nthat produces predictions of the yield curves and their related variances. We refer to this model\nas YC ATT DE. As discussed in [27], model calibration of the DE model can be performed by\nminimizing the following loss function:\nL(θ) =\nX\nt,i,τ\n\u0014 y(i)\nt (τ) − ˆy(i)\nt (τ)\n(σ(i)\nt (τ))2\n+ log\n\u0000\n(σ(i)\nt (τ))2\u0001\n2\n\u0015\n.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "123", "text": "In this vein, we design a network architecture with two output layers\nthat produces predictions of the yield curves and their related variances. We refer to this model\nas YC ATT DE. As discussed in [27], model calibration of the DE model can be performed by\nminimizing the following loss function:\nL(θ) =\nX\nt,i,τ\n\u0014 y(i)\nt (τ) − ˆy(i)\nt (τ)\n(σ(i)\nt (τ))2\n+ log\n\u0000\n(σ(i)\nt (τ))2\u0001\n2\n\u0015\n.\nThe first component represents the MSE between the prediction and the actual yields, scaled\nby the variance. Meanwhile, the second term acts as a penalization factor for observations with\nnotably high estimated variances. We calibrate the YC ATT DE model on the EIOPA data\nin the same setting used above. Figure 6 shows the standard deviation estimates (ˆ σ(i)\nt (τ))τ ∈M\nassociated to the yield curves for the different countries obtained through the YC ATT DE\nmodel.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "124", "text": "The first component represents the MSE between the prediction and the actual yields, scaled\nby the variance. Meanwhile, the second term acts as a penalization factor for observations with\nnotably high estimated variances. We calibrate the YC ATT DE model on the EIOPA data\nin the same setting used above. Figure 6 shows the standard deviation estimates (ˆ σ(i)\nt (τ))τ ∈M\nassociated to the yield curves for the different countries obtained through the YC ATT DE\nmodel.\nWe notice that larger standard deviations are detected for the yields corresponding to short time\nto maturity in contrast to the yields associated with longer maturities. This observation aligns\nwith intuition, as shorter-term yields are more susceptible to market fluctuations, rendering them\nmore volatile. Additionally, we note that standard deviation estimates are notably higher for\nspecific members of the EIOPA family of yield curves, specifically those linked to Turkey, Russia,\nBrazil, and Mexico. This finding is plausible in light of the economic instability experienced in\nthese countries.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "125", "text": "We notice that larger standard deviations are detected for the yields corresponding to short time\nto maturity in contrast to the yields associated with longer maturities. This observation aligns\nwith intuition, as shorter-term yields are more susceptible to market fluctuations, rendering them\nmore volatile. Additionally, we note that standard deviation estimates are notably higher for\nspecific members of the EIOPA family of yield curves, specifically those linked to Turkey, Russia,\nBrazil, and Mexico. This finding is plausible in light of the economic instability experienced in\nthese countries.\nFigure 7 illustrates the PICP of the YCATT, YC ATT DE, and NSS VAR models across various\ntime-to-maturities. Notably, the NN-based models exhibit significantly superior performance\n22", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "126", "text": "South.Africa South.Korea Taiwan Thailand Turkey United.States\nHong.Kong India Japan Malaysia Mexico New.Zealand Singapore\nUnited.Kingdom Australia Brazil Canada Chile China Colombia\nLiechtenstein Norway Poland Romania Russia Sweden Switzerland\nEuro Bulgaria Croatia Czech.Republic Denmark Hungary Iceland\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "127", "text": "0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\nMaturity\nstandard deviation\n 2017\n2018\n2019\n2020\n2021\nTime\nFigure 6: (ˆσ(i)\nt (τ))τ ∈M estimates associated to the yields related to the different countries.\n23", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "128", "text": "●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "129", "text": "●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "130", "text": "●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●●●●●●●●●●●●●●●●●●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●\n●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "131", "text": "●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●●●●●●●●●●●●●●●●●●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "132", "text": "●\n●\n●\n●\n●\n●\n●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●●●\n●\n●●●\n●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●\n●●●●\n●●●●●●\n●●●●●●\n●●●●●●●●●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "133", "text": "●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●●●\n●\n●●●\n●●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●\n●●●●\n●●●●●●\n●●●●●●\n●●●●●●●●●●●●●●●●●\n●●\n●●●●●\n●●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●●●●●\n●●●\n●●●●●●●●●●●●●●●●●●\n●●●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●●●●●●\n●", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "134", "text": "●●\n●●●●●●\n●●●●●●●●●●●●●●●●●\n●●\n●●●●●\n●●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●●●●●\n●●●\n●●●●●●●●●●●●●●●●●●\n●●●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●●●●●●\n●\n●●\n●●\n●\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n50\n100\n150\nMaturity\nPICP\nModel\n●\n●\n●\nNSS_VAR\nYC_ATT\nYC_ATT_DE\nFigure 7: PICP of the NSS VAR, YC ATT, and YC ATT DE models for different time-to-\nmaturities.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "135", "text": "●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●●●●●●●\n●\n●●\n●●\n●\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n50\n100\n150\nMaturity\nPICP\nModel\n●\n●\n●\nNSS_VAR\nYC_ATT\nYC_ATT_DE\nFigure 7: PICP of the NSS VAR, YC ATT, and YC ATT DE models for different time-to-\nmaturities.\ncompared to the NSS VAR model, confirming once again that the NSS model is not sufficiently\nflexible to capture uncertainty in certain yield curve families. On the other hand, YC ATT and\nYC ATT DE emerge as more promising candidates to address this task, producing higher PICP\nfor all the maturities considered.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "136", "text": "YC ATT, and YC ATT DE models for different time-to-\nmaturities.\ncompared to the NSS VAR model, confirming once again that the NSS model is not sufficiently\nflexible to capture uncertainty in certain yield curve families. On the other hand, YC ATT and\nYC ATT DE emerge as more promising candidates to address this task, producing higher PICP\nfor all the maturities considered. Furthermore, upon comparing YC ATT and YC ATT DE,\nwe observe that the former tends to excel for short time-to-maturity, while the latter yields\nhigher PICPs for longer times to maturity. This finding suggests that the Gaussian distribution\nassumption appears to be more suitable for yields with long maturities, since the yields related\nto short maturity are more susceptible to market fluctuations and may be affected by some\nasymmetry and fat tails.\nWe note that the forecast term structures of standard deviations from the YC ATT DE are\na nice by-product of this method, and can be used for other quantitative risk management\napplications.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "137", "text": "This finding suggests that the Gaussian distribution\nassumption appears to be more suitable for yields with long maturities, since the yields related\nto short maturity are more susceptible to market fluctuations and may be affected by some\nasymmetry and fat tails.\nWe note that the forecast term structures of standard deviations from the YC ATT DE are\na nice by-product of this method, and can be used for other quantitative risk management\napplications.\n6.2 Transfer Learning\nThe calibrated YC ATT models may also provide some benefits when used on smaller datasets\nthrough the mechanism of transfer learning. Transfer learning allows for leveraging knowledge\ngained from solving one task and applying it to improve the performance of a different but related\ntask. In other words, we take a model trained on one task (the source task) is repurposed or\nfine-tuned for a different but related task (the target task).\nFor this particular application, we aim to exploit a model with experience acquired in modeling\nand forecasting EIOPA yield curves to construct forecasting models for different families of yield\ncurves.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "138", "text": "Transfer learning allows for leveraging knowledge\ngained from solving one task and applying it to improve the performance of a different but related\ntask. In other words, we take a model trained on one task (the source task) is repurposed or\nfine-tuned for a different but related task (the target task).\nFor this particular application, we aim to exploit a model with experience acquired in modeling\nand forecasting EIOPA yield curves to construct forecasting models for different families of yield\ncurves. Transfer learning is of particular interest when a dataset of experience is available, that\nis too small to calibrate reliable models on. For example, with the recent implementation of\n24", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "139", "text": "IFRS 17, companies will produce portfolio specific illiquidity-adjusted yield curves. It is likely\nthat these curves comprise too small a dataset to model; in this case transfer learning can be\nused.\nTo explore this idea, we collected a new, smaller dataset of US spot curves relating to assets\nwith different rating levels. A visual representation of this supplementary data is presented\nin Figure 11 in the appendix. In this context, the set of yield curve families is defined as\n˙I = {AAA, AA, A, BBB, BB, B}. The set of maturities is represented by\n˙M = {0.25, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30}, denoting ˙M = | ˙M| its cardinality, and\nthe time span ˙T covers monthly observations from January 2015 to October 2021.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "140", "text": "The set of maturities is represented by\n˙M = {0.25, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30}, denoting ˙M = | ˙M| its cardinality, and\nthe time span ˙T covers monthly observations from January 2015 to October 2021. Importantly,\nwe here have different inputs to the networks, both in terms of the number of maturities and\nthe categorical input i. We divided the dataset into a learning sample and a testing sample ,\nconducting a forecasting exercise for the most recent 12 months of experience in this dataset.\nAs noted, the two groups of curves present different number of maturities ( ˙M ̸= M), and\ndirectly applying the YC ATT model to the new data becomes unfeasible.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "141", "text": "Importantly,\nwe here have different inputs to the networks, both in terms of the number of maturities and\nthe categorical input i. We divided the dataset into a learning sample and a testing sample ,\nconducting a forecasting exercise for the most recent 12 months of experience in this dataset.\nAs noted, the two groups of curves present different number of maturities ( ˙M ̸= M), and\ndirectly applying the YC ATT model to the new data becomes unfeasible. To address this\nissue, we equip YC ATT model with an additional layer designed to align the US credit curves\nwith the same dimension as the EIOPA curves. This adjustment enables us to process them\nusing pre-calibrated attention layer of the YC ATT model, facilitating the extraction of the\nrelevant features x(i)\nt .", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "142", "text": "As noted, the two groups of curves present different number of maturities ( ˙M ̸= M), and\ndirectly applying the YC ATT model to the new data becomes unfeasible. To address this\nissue, we equip YC ATT model with an additional layer designed to align the US credit curves\nwith the same dimension as the EIOPA curves. This adjustment enables us to process them\nusing pre-calibrated attention layer of the YC ATT model, facilitating the extraction of the\nrelevant features x(i)\nt . Denoting as ˜Y (i)\nt−L,t ∈ R(L+1)× ˙M the matrix of past yield curves related to\nthe L + 1 previous date, we apply the (learned) mapping:\n˙z : R(L+1)× ˙M → R(L+1)×M , E (i)\nt−L,t = ˙z( ˙Y (i)\nt−L,t);\nthis used an FCN layer.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "143", "text": "Denoting as ˜Y (i)\nt−L,t ∈ R(L+1)× ˙M the matrix of past yield curves related to\nthe L + 1 previous date, we apply the (learned) mapping:\n˙z : R(L+1)× ˙M → R(L+1)×M , E (i)\nt−L,t = ˙z( ˙Y (i)\nt−L,t);\nthis used an FCN layer. Furthermore, since we are now considering a different set of yield curve\nfamilies, we also introduce a new embedding layer aimed to learn a R-valued represetation of\nthe elements in ˙I that is optimal with respect to the forecasting task. It is a mapping with the\nstructure\n˙e ˙I : ˙I → Rq ˙I .\nwhere q ˙I ∈ N is the hyperparameter defining the size of the embedding layer.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "144", "text": "Furthermore, since we are now considering a different set of yield curve\nfamilies, we also introduce a new embedding layer aimed to learn a R-valued represetation of\nthe elements in ˙I that is optimal with respect to the forecasting task. It is a mapping with the\nstructure\n˙e ˙I : ˙I → Rq ˙I .\nwhere q ˙I ∈ N is the hyperparameter defining the size of the embedding layer. In this case, the\nthree output layers related to the calculation of the lower quantiles, the best estimates, and the\nupper quantiles have size equal to ˙M.\nLetting ˙θ be the vector of NN parameters of these two new layers, the calibration process is\ncarried out by minimizing the loss, as defined in Equation 4.4, which now also depends on the\nparameters ˙θ.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "145", "text": "where q ˙I ∈ N is the hyperparameter defining the size of the embedding layer. In this case, the\nthree output layers related to the calculation of the lower quantiles, the best estimates, and the\nupper quantiles have size equal to ˙M.\nLetting ˙θ be the vector of NN parameters of these two new layers, the calibration process is\ncarried out by minimizing the loss, as defined in Equation 4.4, which now also depends on the\nparameters ˙θ. The objective is to learn an effective mapping that transforms the US credit\ncurve data into the dimension of the EIOPA yield curve data in order to be processed by the\npretrained attention layer of the YC ATT model and simultaneously train the new embedding\nlayer. In essence, we optimise the model with respect to ˙θ while keeping constant (or “frozen”))\nthe parameters θ(AT T) related to the key, query and value FCNs and the attention layer:\narg min\n˙θ\nL( ˙θ, θ(AT T)).", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "146", "text": "The objective is to learn an effective mapping that transforms the US credit\ncurve data into the dimension of the EIOPA yield curve data in order to be processed by the\npretrained attention layer of the YC ATT model and simultaneously train the new embedding\nlayer. In essence, we optimise the model with respect to ˙θ while keeping constant (or “frozen”))\nthe parameters θ(AT T) related to the key, query and value FCNs and the attention layer:\narg min\n˙θ\nL( ˙θ, θ(AT T)).\nTo benchmark our model with transfer learning - called YC transfer in the below - we present\nthe comparison against the NS and NSS models. Since we are considering a different set of\ndata, we now extend again the comparison to all four versions of the NS and NSS models with\nboth AR(1) and VAR(1) parameter forecasts. We also include in the comparison the model\nY C AT T that is directly trained on the US credit curve data. For both NN-based models we\n25", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "147", "text": "Model MSE MAE PICP MPIW\nNS AR 0.4605 0.5279 0.5243 0.9221\nNS VAR 0.3273 0.4412 0.43056 0.5796\nNSS AR 0.4833 0.5381 0.5512 1.0109\nNSS VAR 0.3416 0.4514 0.4323 0.8853\nYC ATTγ=1 0.3178 0.4691 0.9003 2.3189\nYC ATTγ=1 (ensemble) 0.3113 0.4640 0.9019 2.3189\nYC transferγ=1 0.3152 0.4622 0.9285 2.1109\nYC transferγ=1 (ensemble) 0.2262 0.3963 0.9852 2.1109\nTable 3: MSE, MAE, PICP and MPIW of the different models considered.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "148", "text": "Bold indicates the\nsmallest value, or, for the PICP, the value closest to α = 0.95.\nalso investigate the use of the ensemble mechanism. In this application, we focus on the case\nγ = 1, i.e., calibrating the best-estimates output of the model using the MAE.\nTable 1 presents the performance metrics for all the models across the four measures. We note\nthat NS VAR and NSS VAR models outperform their counterparts that are based on indepen-\ndent AR models, in terms of point forecasts. However, it is noteworthy that all four models\nexhibit poor performance in terms of PICP, indicating a limited ability to capture uncertainty\nsurrounding future yields. When examining NN-based models, we also note the ensemble mech-\nanism consistently enhances the results of both the YC ATTγ=1 and YC transferγ=1 models.\nThe most accurate outcomes are obtained with the YC transferγ=1 (ensemble), which produces\nsuperior performance in terms of MSE, MAE, and PICP.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "149", "text": "However, it is noteworthy that all four models\nexhibit poor performance in terms of PICP, indicating a limited ability to capture uncertainty\nsurrounding future yields. When examining NN-based models, we also note the ensemble mech-\nanism consistently enhances the results of both the YC ATTγ=1 and YC transferγ=1 models.\nThe most accurate outcomes are obtained with the YC transferγ=1 (ensemble), which produces\nsuperior performance in terms of MSE, MAE, and PICP. Figure 8 illustrates the point and in-\nterval forecasts of the YC ATTγ=1 and the YC transferγ=1 models on three distinct dates: the\nstarting date, the middle date, and the last date of the forecasting horizon. Notably, the width\nof the forecast interval expands as we transition from yield curves associated with high AAA\nratings to those with lower B ratings. This evidence works for all three dates. This trend aligns\nwith expectations, as greater uncertainty is logically anticipated in yields linked to lower-rated\ncompanies.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "150", "text": "Figure 8 illustrates the point and in-\nterval forecasts of the YC ATTγ=1 and the YC transferγ=1 models on three distinct dates: the\nstarting date, the middle date, and the last date of the forecasting horizon. Notably, the width\nof the forecast interval expands as we transition from yield curves associated with high AAA\nratings to those with lower B ratings. This evidence works for all three dates. This trend aligns\nwith expectations, as greater uncertainty is logically anticipated in yields linked to lower-rated\ncompanies. Moreover, both models exhibit commendable performance in predicting yields for\nreliable ratings ( BBB, A, AA, AAA ). The transfer model, in particular, demonstrates enhanced\ncoverage for the lower-rated categories ( B and BB), where more uncertainty is expected. A\nplausible explanation for this observation is that the transfer model adeptly captures tension\nby leveraging insights gained from EIOPA data, featuring yield curves marked by substantial\nvolatility.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "151", "text": "This evidence works for all three dates. This trend aligns\nwith expectations, as greater uncertainty is logically anticipated in yields linked to lower-rated\ncompanies. Moreover, both models exhibit commendable performance in predicting yields for\nreliable ratings ( BBB, A, AA, AAA ). The transfer model, in particular, demonstrates enhanced\ncoverage for the lower-rated categories ( B and BB), where more uncertainty is expected. A\nplausible explanation for this observation is that the transfer model adeptly captures tension\nby leveraging insights gained from EIOPA data, featuring yield curves marked by substantial\nvolatility.\n7 Conclusions\nThe accurate modeling of the yield curves is crucial in insurance and finance for several reasons,\nplaying a fundamental role in risk management, investment decision-making, and asset-liabilities\nevaluation. This paper has advanced the field by developing deep learning models to describe\nthe dynamics of multiple yield curves associated with diverse credit qualities or countries si-\nmultaneously. We have confirmed the intrinsic ability of these models to effectively describe\nlarge-dimensional time-series data and model non-linearity inherent in yield curve dynamics.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "152", "text": "7 Conclusions\nThe accurate modeling of the yield curves is crucial in insurance and finance for several reasons,\nplaying a fundamental role in risk management, investment decision-making, and asset-liabilities\nevaluation. This paper has advanced the field by developing deep learning models to describe\nthe dynamics of multiple yield curves associated with diverse credit qualities or countries si-\nmultaneously. We have confirmed the intrinsic ability of these models to effectively describe\nlarge-dimensional time-series data and model non-linearity inherent in yield curve dynamics.\nOur study shows that these models outperform other well-known models such as the dynamic\nversion of the Nelson and Siegel [26, 9] and the related Svenson extension [38] in the multiple\n26", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "153", "text": "AAA AA A BBB BB B\n2020−11−282021−04−292021−10−29\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0.0\n2.5\n5.0\n7.5\n0.0\n2.5\n5.0\n7.5\n0.0\n2.5\n5.0\n7.5\nMaturity\nvalue\nModel\nactual data\nYC_ATT\nYC_transfer\nFigure 8: Point and interval forecasts for the US credit curves generated by the YC ATT and YC transfer models for the central, the middle\nand the final date of forecasting period.\n27", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "154", "text": "yield curve modeling and forecasting tasks. We performed several numerical experiments on\nthe data provided by the European Insurance and Occupational Pensions Authority (EIOPA).\nAlthough different kinds of neural network architecture have been investigated, we discover the\nmost promising results have been obtained by using the self-attention mechanism, which has\nproven successful in natural language processing. Furthermore, we also investigate techniques\nfor quantifying the uncertainty around predictions, a critical yet under-explored in the existing\nliterature. We explored the use of nonparametric quantile regression and designed an archi-\ntecture specifically designed to avoid quantile crossing issues. The numerical analysis of the\nperformance, conducted in terms of prediction interval coverage probability and mean predic-\ntion interval width, shows the effectiveness of the proposed approach. We finally discuss two\npossible extensions and variants of the proposed model. The first one considers using the deep\nensemble method for measuring uncertainty in forecasts.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "155", "text": "Furthermore, we also investigate techniques\nfor quantifying the uncertainty around predictions, a critical yet under-explored in the existing\nliterature. We explored the use of nonparametric quantile regression and designed an archi-\ntecture specifically designed to avoid quantile crossing issues. The numerical analysis of the\nperformance, conducted in terms of prediction interval coverage probability and mean predic-\ntion interval width, shows the effectiveness of the proposed approach. We finally discuss two\npossible extensions and variants of the proposed model. The first one considers using the deep\nensemble method for measuring uncertainty in forecasts. This approach, which requires the\nassumption of heteroscedastic Gaussian distributions of the response, appears to be promising,\nespecially in describing the dynamics of yields with long time-to-maturities; the DE approach\nalso produces a term structure of forecast volatility, which is a useful by-product. The second\nextension uses a transfer learning mechanism that could be useful to exploit the experiences\ngained in modeling the EIOPA yield curves to improve the performance of a model related to a\ndifferent set of yield curves.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "156", "text": "The first one considers using the deep\nensemble method for measuring uncertainty in forecasts. This approach, which requires the\nassumption of heteroscedastic Gaussian distributions of the response, appears to be promising,\nespecially in describing the dynamics of yields with long time-to-maturities; the DE approach\nalso produces a term structure of forecast volatility, which is a useful by-product. The second\nextension uses a transfer learning mechanism that could be useful to exploit the experiences\ngained in modeling the EIOPA yield curves to improve the performance of a model related to a\ndifferent set of yield curves. A numerical illustration of this approach is conducted considering\nthe US credit curve data with different credit qualities (ratings). We show that this approach\nallows for improved forecasting performance, especially in terms of prediction intervals, when\nthe data we are considering are subject to more uncertainty.\nIn future studies, we plan to explore the use of explainable deep learning techniques, in particular,\nthe LocalGLMnet model introduced in [35], to model effectively the uncertainty inherent in\nfuture yield predictions in an intepretable manner.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "157", "text": "A numerical illustration of this approach is conducted considering\nthe US credit curve data with different credit qualities (ratings). We show that this approach\nallows for improved forecasting performance, especially in terms of prediction intervals, when\nthe data we are considering are subject to more uncertainty.\nIn future studies, we plan to explore the use of explainable deep learning techniques, in particular,\nthe LocalGLMnet model introduced in [35], to model effectively the uncertainty inherent in\nfuture yield predictions in an intepretable manner. Initially designed for expected values, an\nextension to quantile modeling is interesting but also challenging due to the intricate issue of\nquantile crossing. Moreover, we would like to model jointly interest rates and other market\nvariables using a similar model. Finally, our future research agenda extends to the investigation\nof other potential applications of attention and transfer models within the insurance domain.\nSpecifically, we aim to explore their efficacy in non-life insurance fields, such as frequency-severity\nmodeling.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "158", "text": "Initially designed for expected values, an\nextension to quantile modeling is interesting but also challenging due to the intricate issue of\nquantile crossing. Moreover, we would like to model jointly interest rates and other market\nvariables using a similar model. Finally, our future research agenda extends to the investigation\nof other potential applications of attention and transfer models within the insurance domain.\nSpecifically, we aim to explore their efficacy in non-life insurance fields, such as frequency-severity\nmodeling.\nAcknowledgements\nThe authors acknowledge the International Actuarial Association that financially supported this\nwork through the “Life Section Research Grant” assigned to the project “Multiple Yield Curve\nmodeling and Forecasting using Deep Learning”. The authors are grateful to the staff members\nat Old Mutual who provided the US credit curves.\nReferences\n[1] Aljinovi´c, Z., and Poklepovi ´c, T. Neural networks and vector autoregressive model\nin forecasting yield curve. In The 6th International Conference on Information Technology\n(ICIT) (2013), pp. 1–8.\n28", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "159", "text": "[2] Atkins, P. J., and Cummins, M. Improved scalability and risk factor proxying with\na two-step principal component analysis for multi-curve modelling. European Journal of\nOperational Research 304, 3 (2023), 1331–1348.\n[3] Barigou, K., and Delong,  L.Pricing equity-linked life insurance contracts with multiple\nrisk factors by neural networks. Journal of Computational and Applied Mathematics 404\n(2022), 113922.\n[4] Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and\nnew perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35 , 8\n(2013), 1798–1828.\n[5] Bliss, R. R. Testing term structure estimation methods. Tech. rep., Working Paper, 1996.\n[6] Bowsher, C. G., and Meeks, R. The dynamics of economic functions: modeling and\nforecasting the yield curve.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "160", "text": "[4] Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and\nnew perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35 , 8\n(2013), 1798–1828.\n[5] Bliss, R. R. Testing term structure estimation methods. Tech. rep., Working Paper, 1996.\n[6] Bowsher, C. G., and Meeks, R. The dynamics of economic functions: modeling and\nforecasting the yield curve. Journal of the American Statistical Association 103, 484 (2008),\n1419–1437.\n[7] Cuchiero, C., Fontana, C., and Gnoatto, A. A general HJM framework for multiple\nyield curve modelling. Finance and Stochastics 20 (2016), 267–320.\n[8] De Rezende, R. B., and Ferreira, M. S. Modeling and forecasting the yield curve by\nan extended Nelson-Siegel class of models: A quantile autoregression approach.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "161", "text": "Journal of the American Statistical Association 103, 484 (2008),\n1419–1437.\n[7] Cuchiero, C., Fontana, C., and Gnoatto, A. A general HJM framework for multiple\nyield curve modelling. Finance and Stochastics 20 (2016), 267–320.\n[8] De Rezende, R. B., and Ferreira, M. S. Modeling and forecasting the yield curve by\nan extended Nelson-Siegel class of models: A quantile autoregression approach. Journal of\nForecasting 32, 2 (2013), 111–123.\n[9] Diebold, F. X., and Li, C. Forecasting the term structure of government bond yields.\nJournal of Econometrics 130 , 2 (2006), 337–364.\n[10] Diebold, F. X., and Rudebusch, G. D. Yield curve modeling and forecasting: the\ndynamic Nelson-Siegel approach. Princeton University Press, 2013.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "162", "text": "Journal of\nForecasting 32, 2 (2013), 111–123.\n[9] Diebold, F. X., and Li, C. Forecasting the term structure of government bond yields.\nJournal of Econometrics 130 , 2 (2006), 337–364.\n[10] Diebold, F. X., and Rudebusch, G. D. Yield curve modeling and forecasting: the\ndynamic Nelson-Siegel approach. Princeton University Press, 2013.\n[11] Diebold, F. X., Rudebusch, G. D., and Aruoba, S. B. The macroeconomy and the\nyield curve: a dynamic latent factor approach. Journal of econometrics 131 , 1-2 (2006),\n309–338.\n[12] Duffee, G. R. Term premia and interest rate forecasts in affine models. The Journal of\nFinance 57, 1 (2002), 405–443.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "163", "text": "Princeton University Press, 2013.\n[11] Diebold, F. X., Rudebusch, G. D., and Aruoba, S. B. The macroeconomy and the\nyield curve: a dynamic latent factor approach. Journal of econometrics 131 , 1-2 (2006),\n309–338.\n[12] Duffee, G. R. Term premia and interest rate forecasts in affine models. The Journal of\nFinance 57, 1 (2002), 405–443.\n[13] Fama, E. F., and Bliss, R. R. The information in long-maturity forward rates. The\nAmerican Economic Review (1987), 680–692.\n[14] Gabrielli, A. A neural network boosted double overdispersed Poisson claims reserving\nmodel. ASTIN Bulletin: The Journal of the IAA 50 , 1 (2020), 25–60.\n[15] Gerhart, C., and L ¨utkebohmert, E. Empirical analysis and forecasting of multiple\nyield curves.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "164", "text": "[13] Fama, E. F., and Bliss, R. R. The information in long-maturity forward rates. The\nAmerican Economic Review (1987), 680–692.\n[14] Gabrielli, A. A neural network boosted double overdispersed Poisson claims reserving\nmodel. ASTIN Bulletin: The Journal of the IAA 50 , 1 (2020), 25–60.\n[15] Gerhart, C., and L ¨utkebohmert, E. Empirical analysis and forecasting of multiple\nyield curves. Insurance: Mathematics and Economics 95 (2020), 59–78.\n[16] Gerhart, C., L ¨utkebohmert, E., and Weber, M. Robust forecasting of multiple\nyield curves. In Theory and Applications of Time Series Analysis: Selected Contributions\nfrom ITISE 2018 5 (2019), Springer, pp. 187–202.\n29", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "165", "text": "[17] Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT press, 2016.\n[18] Guo, C., and Berkhahn, F. Entity embeddings of categorical variables. arXiv preprint\narXiv:1604.06737 (2016).\n[19] H¨ardle, W. K., and Majer, P. Yield curve modeling and forecasting using semipara-\nmetric factor dynamics. The European Journal of Finance 22 , 12 (2016), 1109–1129.\n[20] Heath, D., Jarrow, R., and Morton, A. Bond pricing and the term structure of\ninterest rates: A new methodology for contingent claims valuation. Econometrica: Journal\nof the Econometric Society (1992), 77–105.\n[21] Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation\n9, 8 (1997), 1735–1780.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "166", "text": "The European Journal of Finance 22 , 12 (2016), 1109–1129.\n[20] Heath, D., Jarrow, R., and Morton, A. Bond pricing and the term structure of\ninterest rates: A new methodology for contingent claims valuation. Econometrica: Journal\nof the Econometric Society (1992), 77–105.\n[21] Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation\n9, 8 (1997), 1735–1780.\n[22] Hull, J., and White, A. Pricing interest-rate-derivative securities. The Review of\nFinancial Studies 3 , 4 (1990), 573–592.\n[23] Kauffmann, P. C., Takada, H. H., Terada, A. T., and Stern, J. M. Learning\nforecast-efficient yield curve factor decompositions with neural networks. Econometrics 10,\n2 (2022), 15.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "167", "text": "Neural computation\n9, 8 (1997), 1735–1780.\n[22] Hull, J., and White, A. Pricing interest-rate-derivative securities. The Review of\nFinancial Studies 3 , 4 (1990), 573–592.\n[23] Kauffmann, P. C., Takada, H. H., Terada, A. T., and Stern, J. M. Learning\nforecast-efficient yield curve factor decompositions with neural networks. Econometrics 10,\n2 (2022), 15.\n[24] Kuo, K., and Richman, R. Embeddings and attention in predictive modeling. arXiv\npreprint arXiv:2104.03545 (2021).\n[25] Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles.Advances in neural information processing\nsystems 30 (2017).", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "168", "text": "Econometrics 10,\n2 (2022), 15.\n[24] Kuo, K., and Richman, R. Embeddings and attention in predictive modeling. arXiv\npreprint arXiv:2104.03545 (2021).\n[25] Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles.Advances in neural information processing\nsystems 30 (2017).\n[26] Nelson, C. R., and Siegel, A. F. Parsimonious modeling of yield curves. Journal of\nBusiness (1987), 473–489.\n[27] Nix, D. A., and Weigend, A. S. Estimating the mean and variance of the target\nprobability distribution. In Proceedings of 1994 IEEE international conference on neural\nnetworks (ICNN’94) (1994), vol. 1, IEEE, pp. 55–60.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "169", "text": "[26] Nelson, C. R., and Siegel, A. F. Parsimonious modeling of yield curves. Journal of\nBusiness (1987), 473–489.\n[27] Nix, D. A., and Weigend, A. S. Estimating the mean and variance of the target\nprobability distribution. In Proceedings of 1994 IEEE international conference on neural\nnetworks (ICNN’94) (1994), vol. 1, IEEE, pp. 55–60.\n[28] Noll, A., Salzmann, R., and Wuthrich, M. V. Case study: French motor third-party\nliability claims. Available at SSRN 3164764 (2020).\n[29] Nunes, M., Gerding, E., McGroarty, F., and Niranjan, M. A comparison of\nmultitask and single task learning with artificial neural networks for yield curve forecasting.\nExpert Systems with Applications 119 (2019), 362–375.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "170", "text": "1, IEEE, pp. 55–60.\n[28] Noll, A., Salzmann, R., and Wuthrich, M. V. Case study: French motor third-party\nliability claims. Available at SSRN 3164764 (2020).\n[29] Nunes, M., Gerding, E., McGroarty, F., and Niranjan, M. A comparison of\nmultitask and single task learning with artificial neural networks for yield curve forecasting.\nExpert Systems with Applications 119 (2019), 362–375.\n[30] Perla, F., Richman, R., Scognamiglio, S., and W ¨uthrich, M. V. Time-series\nforecasting of mortality rates using deep learning. Scandinavian Actuarial Journal 2021 , 7\n(2021), 572–598.\n[31] Piazzesi, M. Affine term structure models. In Handbook of financial econometrics: Tools\nand Techniques. Elsevier, 2010, pp. 691–766.\n30", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "171", "text": "[32] Redfern, D., and McLean, D. Principal component analysis for yield curve modelling.\nEnterprise Risk Solutions (2014).\n[33] Richman, R. Ai in actuarial science–a review of recent advances–part 1. Annals of Actu-\narial Science 15 , 2 (2021), 207–229.\n[34] Richman, R. Ai in actuarial science–a review of recent advances–part 2. Annals of Actu-\narial Science 15 , 2 (2021), 230–258.\n[35] Richman, R., and W ¨uthrich, M. V. LocalGLMnet: interpretable deep learning for\ntabular data. Scandinavian Actuarial Journal 2023 , 1 (2023), 71–95.\n[36] Scognamiglio, S. Calibrating the Lee-Carter and the Poisson Lee-Carter models via\nneural networks.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "172", "text": "Annals of Actu-\narial Science 15 , 2 (2021), 230–258.\n[35] Richman, R., and W ¨uthrich, M. V. LocalGLMnet: interpretable deep learning for\ntabular data. Scandinavian Actuarial Journal 2023 , 1 (2023), 71–95.\n[36] Scognamiglio, S. Calibrating the Lee-Carter and the Poisson Lee-Carter models via\nneural networks. ASTIN Bulletin: The Journal of the IAA 52 , 2 (2022), 519–561.\n[37] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,\nR. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research 15, 1 (2014), 1929–1958.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "173", "text": "ASTIN Bulletin: The Journal of the IAA 52 , 2 (2022), 519–561.\n[37] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,\nR. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research 15, 1 (2014), 1929–1958.\n[38] Svensson, L. E. Estimating and interpreting forward interest rates: Sweden 1992-1994,\n1994.\n[39] Teichmann, J., and W ¨uthrich, M. V. Consistent yield curve prediction. ASTIN\nBulletin: The Journal of the IAA 46 , 2 (2016), 191–224.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "174", "text": "Journal of Machine\nLearning Research 15, 1 (2014), 1929–1958.\n[38] Svensson, L. E. Estimating and interpreting forward interest rates: Sweden 1992-1994,\n1994.\n[39] Teichmann, J., and W ¨uthrich, M. V. Consistent yield curve prediction. ASTIN\nBulletin: The Journal of the IAA 46 , 2 (2016), 191–224.\n[40] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\nKaiser,  L., and Polosukhin, I. Attention is all you need. Advances in Neural Informa-\ntion Processing Systems 30 (2017).\n[41] Venter, G. G. Testing distributions of stochastically generated yield curves. ASTIN\nBulletin: The Journal of the IAA 34 , 1 (2004), 229–247.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "175", "text": "[40] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\nKaiser,  L., and Polosukhin, I. Attention is all you need. Advances in Neural Informa-\ntion Processing Systems 30 (2017).\n[41] Venter, G. G. Testing distributions of stochastically generated yield curves. ASTIN\nBulletin: The Journal of the IAA 34 , 1 (2004), 229–247.\n[42] W¨uthrich, M. V., and Merz, M. Statistical Foundations of Actuarial Learning and its\nApplications. Springer Actuarial, 2023.\n[43] Yasuoka, T. Interest Rate Modeling for Risk Management: Market Price of Interest Rate\nRisk, vol. 1. Bentham Science Publishers, 2018.\n31", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "176", "text": "A Appendix: Proof, data and plots\nHere we show that the average MPIW across different training attempts coincides with the\nMPIW of the ensemble predictions obtained by averaging the predictions of the ten models.\nWe denote:\n• y(l)\ni,k lower bound of the k-th model, in the i-th observation.\n• y(u)\ni,k upper bound of the k-th model, in the i-th observation.\n• i = 1, . . . , n is the index related to the different observations;\n• k = 1, . . . , m is the index related to the different models.\nThe MPIW of the k-th model on the different data points is:\nMPIWk = 1\nn\nnX\ni\n(y(u)\ni,k − y(l)\ni,k).", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "177", "text": "• y(u)\ni,k upper bound of the k-th model, in the i-th observation.\n• i = 1, . . . , n is the index related to the different observations;\n• k = 1, . . . , m is the index related to the different models.\nThe MPIW of the k-th model on the different data points is:\nMPIWk = 1\nn\nnX\ni\n(y(u)\ni,k − y(l)\ni,k).\nThe average MPIW is:\nMPIWaverage = 1\nm\nmX\nk=1\nMPIWk = 1\nn ∗ m\nmX\nk=1\nnX\ni=1\n(y(u)\ni,k − y(l)\ni,k)\nthe MPIW of the ensemble predictions is:\nMPIWensemble = 1\nn\nnX\ni=1\n \n1\nm\nmX\nk=1\ny(u)\ni,k − 1\nm\nmX\nk=1\ny(l)\ni,k\n!", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "178", "text": "The average MPIW is:\nMPIWaverage = 1\nm\nmX\nk=1\nMPIWk = 1\nn ∗ m\nmX\nk=1\nnX\ni=1\n(y(u)\ni,k − y(l)\ni,k)\nthe MPIW of the ensemble predictions is:\nMPIWensemble = 1\nn\nnX\ni=1\n \n1\nm\nmX\nk=1\ny(u)\ni,k − 1\nm\nmX\nk=1\ny(l)\ni,k\n!\n= 1\nn ∗ m\nmX\nk=1\nnX\ni=1\n(y(u)\ni,k − y(l)\ni,k).\nThen, we can conclude that:\nMPIWaverage = MPIWensemble.\n32", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "179", "text": "South.Africa South.Korea Taiwan Thailand Turkey United.States\nHong.Kong India Japan Malaysia Mexico New.Zealand Singapore\nUnited.Kingdom Australia Brazil Canada Chile China Colombia\nLiechtenstein Norway Poland Romania Russia Sweden Switzerland\nEuro Bulgaria Croatia Czech.Republic Denmark Hungary Iceland\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0\n50\n100\n150\n0.02\n0.04\n0.06\n0.08\n−0.01\n0.00\n0.01\n0.02\n0.02\n0.04\n0.06\n0.08\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.01\n0.02\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "180", "text": "02\n0.04\n0.06\n0.08\n−0.01\n0.00\n0.01\n0.02\n0.02\n0.04\n0.06\n0.08\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.04\n0.06\n0.08\n0.10\n0.1\n0.2\n0.3\n0.00\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "181", "text": "01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.04\n0.06\n0.08\n0.10\n0.1\n0.2\n0.3\n0.00\n0.01\n0.02\n0.03\n0.04\n0.02\n0.04\n0.06\n0.00\n0.01\n0.02\n0.03\n0.04\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "182", "text": "10\n0.1\n0.2\n0.3\n0.00\n0.01\n0.02\n0.03\n0.04\n0.02\n0.04\n0.06\n0.00\n0.01\n0.02\n0.03\n0.04\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.10\n0.15\n0.00\n0.01\n0.02\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "183", "text": "00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.10\n0.15\n0.00\n0.01\n0.02\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "184", "text": "03\n0.04\n0.00\n0.01\n0.02\n0.03\n0.04\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.01\n0.02\n0.03\n0.04\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.04\n0.04\n0.06\n0.08\n0.10\nMaturity\nvalue\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nY ear\nFigure 9: Risk-free interest rate term structures derived from government bonds of different countries;observation period spans from December\n2015 to December 2021.\n33", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "185", "text": "●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\nPICP MPIW\nMSE MAE\nYC_ATT\nYC_CONV\nYC_LSTM\nYC_TRAN\nYC_ATT\nYC_CONV\nYC_LSTM\nYC_TRAN\n0.3\n0.4\n0.5\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.3\n0.4\n0.5\n0.4\n0.6\n0.8\nModel\nvalue\nloss_training\nmae\nmse\nFigure 10: Boxplot of the out-of-sample MSE, MAE, PICP and MPIW of the different models\non ten runs; the MSE values are multiplied 10 5, the MAE values are multiplied by 10 2.", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
{"id": "186", "text": "AAA AA A BBB BB B\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n0.0\n2.5\n5.0\n7.5\nMaturity\nvalue\n2016\n2018\n2020\nY ear\nFigure 11: US credit curves related to different rating qualities.\n34", "metadata": {"file_path": "data/2401.16985v1.pdf", "file_name": "2401.16985v1.pdf", "creation_date": "2026-01-11", "last_modified_date": "2026-01-07"}}
